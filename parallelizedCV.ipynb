{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Imports ########\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# Converting labels to 1-Hot Vectors\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "##########################\n",
    "\n",
    "root_logdir = \"./tf_logs\"\n",
    "datadir = \"data/\"\n",
    "figures_dir = \"data/figures/\"\n",
    "\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "\n",
    "np.random.seed(seed=42)\n",
    "\n",
    "from helper import *\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Half the data will be split out as validation and 0.2 as the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "'''\n",
    "Expects data to be 2D numpy array\n",
    "'''\n",
    "def calculateEntropy(data, plot=False):\n",
    "    from scipy.stats import entropy\n",
    "    \n",
    "    nsamples = len(data)\n",
    "    nbins = 20\n",
    "\n",
    "    xedges = np.linspace(0,15,nbins+1)\n",
    "    yedges = np.linspace(0,15,nbins+1)\n",
    "    \n",
    "    x = np.clip(data[:,0], xedges[0], xedges[-1])\n",
    "    y = np.clip(data[:,1], yedges[0], yedges[-1])\n",
    "    \n",
    "    grid, xedges, yedges = np.histogram2d(x, y, bins=[xedges,yedges])\n",
    "    densities = (grid/nsamples).flatten()\n",
    "    \n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(1, figsize=(14, 14))\n",
    "\n",
    "        ax.imshow(grid, interpolation='nearest', origin='low',\n",
    "                    extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]], cmap=\"jet\")\n",
    "#         plt.colorbar()\n",
    "        plt.show()\n",
    "    \n",
    "    return entropy(densities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dnn(num_features, num_nodes = 16, depth = 2, num_labels=2, activation = \"elu\"):\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    import keras\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    nn = keras.models.Sequential()\n",
    "    Dense = keras.layers.Dense\n",
    "    \n",
    "    # Using He initialization\n",
    "    he_init = tf.keras.initializers.he_uniform()\n",
    "    \n",
    "    nn.add(Dense(units = num_nodes, activation=activation, input_dim=num_features,\n",
    "                kernel_initializer=he_init))\n",
    "    \n",
    "    for i in range(1,depth):\n",
    "        nn.add(Dense(units = num_nodes, activation=activation,\n",
    "                    kernel_initializer=he_init))\n",
    "\n",
    "    nn.add(Dense(units=num_labels, activation= \"softmax\",\n",
    "                kernel_initializer=he_init))\n",
    "    \n",
    "    nn.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer='sgd',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return nn\n",
    "\n",
    "def train_model(model, X, y, epochs=30, batch_size=20, verbose=0):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    \n",
    "    ZScaler = StandardScaler().fit(X)\n",
    "    \n",
    "    X_train = ZScaler.transform(X)\n",
    "    y_train = hot_encoder.transform(y)\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size = batch_size, verbose=verbose)\n",
    "    \n",
    "    return history, ZScaler\n",
    "\n",
    "'''\n",
    "Assumes categorical output from DNN\n",
    "'''\n",
    "def getCorrectPredictions(model, samples, labels, enc):\n",
    "    import numpy as np\n",
    "    \n",
    "    predictions = model.predict(samples)\n",
    "    preds = np.array([np.argmax(x) for x in predictions])\n",
    "    true_labels = np.array([x for x in labels])\n",
    "\n",
    "    correct = preds == true_labels\n",
    "    \n",
    "    return samples[correct], labels[correct], correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94476c8c65df46889bc96d80683eb906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "from time import time\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "depth_epochs = [(2,50),(3,50)]\n",
    "# depth_epochs = [(2,50),(3,50),(4,100),(5,100), (5,200), (6,100)]\n",
    "\n",
    "original_data, modded_samples, training_labels, original_labels = simulate_blobs(class_size=3000)\n",
    "X, y, y_original, X_valid, y_valid, y_valid_original = split_valid(modded_samples, original_labels, training_labels)\n",
    "\n",
    "num_features = X.shape[1]\n",
    "\n",
    "hot_encoder = dfHotEncoder()\n",
    "hot_encoder.fit(y)\n",
    "\n",
    "\n",
    "def runDNN(depth, epochs, train_data, test_data):\n",
    "    import innvestigate\n",
    "    import innvestigate.utils as iutils\n",
    "    \n",
    "    X_train, y_train = train_data\n",
    "    X_test, y_test = test_data\n",
    "    \n",
    "    nn = build_dnn(num_features)\n",
    "\n",
    "\n",
    "    history, ZScaler = train_model(nn, X_train, y_train, \n",
    "                                   epochs=epochs, batch_size=20)\n",
    "\n",
    "    scaled_samples = ZScaler.transform(X_test)\n",
    "    final_acc = nn.evaluate(scaled_samples,hot_encoder.transform(y_test), verbose=0)\n",
    "\n",
    "    # Getting all the samples that can be correctly predicted\n",
    "    all_samples, _labels, correct_idxs = getCorrectPredictions(nn, scaled_samples, y_test, enc = hot_encoder)\n",
    "\n",
    "    # Stripping the softmax activation from the model\n",
    "    model_w_softmax = nn\n",
    "    model = iutils.keras.graph.model_wo_softmax(model_w_softmax)\n",
    "\n",
    "    # Creating an analyzer\n",
    "    lrp_E = innvestigate.analyzer.relevance_based.relevance_analyzer.LRPEpsilon(model=model, epsilon=1e-3)\n",
    "    lrp_results = lrp_E.analyze(all_samples)\n",
    "\n",
    "    return (final_acc, lrp_results, correct_idxs)\n",
    "\n",
    "def runCV(depth,epoch):\n",
    "    \n",
    "    start_time = time()\n",
    "    \n",
    "    histories = []\n",
    "    testing_indxs =[]\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    cv_original_labels = []\n",
    "    cv_lrp_results = []\n",
    "    zoo = []\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(getKF(X, y_original)):\n",
    "\n",
    "        X_train = X.iloc[train_index]\n",
    "        y_train = y.iloc[train_index]\n",
    "        X_test  = X.iloc[test_index]\n",
    "        y_test = y.iloc[test_index]\n",
    "        y_test_original = y_original.iloc[test_index]\n",
    "\n",
    "        final_acc, lrp_results, lrp_idxs = runDNN(depth, epoch, train_data = [X_train,y_train], test_data = [X_test, y_test])\n",
    "\n",
    "        cv_original_labels.extend(y_test_original[lrp_idxs])\n",
    "        cv_lrp_results.extend(lrp_results)\n",
    "\n",
    "        print(\"Fold {} scores: loss={:0.3f} accuracy={:.4f}\".format(\n",
    "            i, final_acc[0], final_acc[1]))\n",
    "\n",
    "    print(\"{}/{} Runtime: {:.3f}s\".format(depth, epoch, time()-start_time))\n",
    "\n",
    "    return (cv_lrp_results, cv_original_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pool execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Work/anaconda3/envs/condaflow/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.WARNING:tensorflow:From /Users/Work/anaconda3/envs/condaflow/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.WARNING:tensorflow:From /Users/Work/anaconda3/envs/condaflow/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "\n",
      "WARNING:tensorflow:From /Users/Work/anaconda3/envs/condaflow/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.WARNING:tensorflow:From /Users/Work/anaconda3/envs/condaflow/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.WARNING:tensorflow:From /Users/Work/anaconda3/envs/condaflow/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      "\n",
      "WARNING:tensorflow:From /Users/Work/anaconda3/envs/condaflow/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Fold 0 scores: loss=0.001 accuracy=1.0000\n",
      "Fold 1 scores: loss=0.001 accuracy=1.0000\n",
      "WARNING:tensorflow:From /Users/Work/anaconda3/envs/condaflow/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Fold 0 scores: loss=0.000 accuracy=1.0000\n",
      "Fold 2 scores: loss=0.001 accuracy=1.0000\n",
      "WARNING:tensorflow:From /Users/Work/anaconda3/envs/condaflow/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Fold 0 scores: loss=0.000 accuracy=1.0000\n",
      "Fold 3 scores: loss=0.001 accuracy=1.0000\n",
      "Fold 4 scores: loss=0.001 accuracy=1.0000\n",
      "Fold 1 scores: loss=0.000 accuracy=1.0000\n",
      "Fold 5 scores: loss=0.001 accuracy=1.0000\n",
      "Fold 6 scores: loss=0.002 accuracy=1.0000\n",
      "Fold 1 scores: loss=0.000 accuracy=1.0000\n",
      "Fold 2 scores: loss=0.000 accuracy=1.0000\n",
      "Fold 7 scores: loss=0.001 accuracy=1.0000\n",
      "Fold 8 scores: loss=0.001 accuracy=1.0000\n",
      "Fold 9 scores: loss=0.001 accuracy=1.0000\n",
      "3/75 Runtime: 93.990s\n",
      "Fold 3 scores: loss=0.000 accuracy=1.0000\n",
      "Fold 2 scores: loss=0.000 accuracy=1.0000\n",
      "Fold 4 scores: loss=0.000 accuracy=1.0000\n",
      "Fold 3 scores: loss=0.000 accuracy=1.0000\n",
      "Fold 5 scores: loss=0.001 accuracy=1.0000\n",
      "Fold 6 scores: loss=0.001 accuracy=1.0000\n",
      "Fold 4 scores: loss=0.000 accuracy=1.0000\n",
      "Fold 7 scores: loss=0.000 accuracy=1.0000\n",
      "Fold 5 scores: loss=0.001 accuracy=1.0000\n",
      "Fold 8 scores: loss=0.000 accuracy=1.0000\n",
      "Fold 9 scores: loss=0.000 accuracy=1.0000\n",
      "5/200 Runtime: 224.676s\n",
      "Fold 6 scores: loss=0.000 accuracy=1.0000\n",
      "Fold 7 scores: loss=0.000 accuracy=1.0000\n",
      "Fold 8 scores: loss=0.000 accuracy=1.0000\n",
      "Fold 9 scores: loss=0.000 accuracy=1.0000\n",
      "6/300 Runtime: 301.495s\n",
      "Fold 0 scores: loss=0.000 accuracy=1.0000\n",
      "Fold 1 scores: loss=0.000 accuracy=1.0000\n",
      "Fold 2 scores: loss=0.000 accuracy=1.0000\n",
      "Fold 3 scores: loss=0.000 accuracy=1.0000\n",
      "Fold 4 scores: loss=0.000 accuracy=1.0000\n",
      "Fold 5 scores: loss=0.001 accuracy=1.0000\n",
      "Fold 6 scores: loss=0.000 accuracy=1.0000\n",
      "Fold 7 scores: loss=0.000 accuracy=1.0000\n",
      "Fold 8 scores: loss=0.000 accuracy=1.0000\n",
      "Fold 9 scores: loss=0.000 accuracy=1.0000\n",
      "6/300 Runtime: 236.848s\n",
      "Done training DNNs\n"
     ]
    }
   ],
   "source": [
    "depth_epochs = [(3,75), (5,200), (6,300)]\n",
    "# depth_epochs = [(2,50),(3,50)]\n",
    "print(\"Starting pool execution...\")\n",
    "\n",
    "num_procs = 3\n",
    "results = []\n",
    "if __name__ == '__main__':\n",
    "    with Pool(processes = num_procs) as pool:\n",
    "        curr = 0\n",
    "        while curr < len(depth_epochs):\n",
    "            param_list = depth_epochs[curr: curr+num_procs]\n",
    "            curr += num_procs-1\n",
    "        \n",
    "            results.extend(pool.starmap(runCV, [(depth,epoch) for depth,epoch in param_list]))\n",
    "\n",
    "# pool.close()\n",
    "# pool.join()\n",
    "print(\"Done training DNNs\")\n",
    "\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eefd59b8db7f4a7ebc41e115a4b8364f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc161978300746768533a8b83ab02c24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2115ff92ffce443b868c466ce0ba7cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab0de49a6064884b7260198d43304de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_args = {\"kind\":\"scatter\", \"x\":0,  \"y\":1, \"c\":\"label\", \"cmap\": \"Set1\", \"s\":10, \"alpha\":0.25}\n",
    "\n",
    "plt.close(\"Depth Comparison\")\n",
    "fig, axs = plt.subplots(len(depth_epochs),1, figsize=(15,8*len(depth_epochs)), num=\"Depth Comparison\")\n",
    "# plt.tight_layout()\n",
    "\n",
    "entropies = []\n",
    "\n",
    "for i,(d, epochs) in enumerate(depth_epochs):\n",
    "    \n",
    "    lrp_results, lrp_labels = results[i]\n",
    "    \n",
    "    pos_lrp = np.array(lrp_results)\n",
    "    pos_lrp[pos_lrp<0] = 0\n",
    "    data = pos_lrp\n",
    "    lrp_entropy = calculateEntropy(data)\n",
    "    entropies.append(lrp_entropy)\n",
    "    \n",
    "    axs[i].scatter(*data.T, s=50, linewidth=0, c=lrp_labels, alpha=0.5, cmap=plot_args[\"cmap\"])\n",
    "    axs[i].set_title(\"Depth: {}, Epochs: {}\".format(d, epochs))\n",
    "    axs[i].text(0.95,0.95,\"Entropy: {:.4f}\".format(lrp_entropy),\n",
    "            horizontalalignment='right', verticalalignment='top',\n",
    "            fontsize=14, transform=axs[i].transAxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.savefig(figures_dir + \"dCV5_6_6.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condaflow",
   "language": "python",
   "name": "condaflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
