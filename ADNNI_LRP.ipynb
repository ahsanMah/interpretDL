{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "import sys\n",
    "# sys.path.append(\"/Users/Work/Developer/interpretDL/interprettensor\")\n",
    "root_logdir = \"./tf_logs\"\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib widget\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "######### Taken from sklearn #######\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=[8,8])\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "def get1hot(y_train,y_test):\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "    enc = OneHotEncoder(categories=\"auto\", sparse=False)\n",
    "    y_train_1hot = enc.fit_transform([[label] for label in y_train]) # Since the function expects an array of \"features\" per sample\n",
    "    y_test_1hot = enc.fit_transform([[label] for label in y_test])\n",
    "\n",
    "    return y_train_1hot, y_test_1hot\n",
    "\n",
    "def get_split(features, labels):\n",
    "    features = np.array(features)\n",
    "    # The train set will have equal amounts of each target class\n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    for train_index, test_index in split.split(features, labels):\n",
    "        X_train = features[train_index]\n",
    "        y_train = labels.iloc[train_index]\n",
    "        X_test = features[test_index]\n",
    "        y_test = labels.iloc[test_index]\n",
    "        \n",
    "        yield X_train, y_train, X_test, y_test\n",
    "\n",
    "def plot_history(history):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(18,6))\n",
    "    \n",
    "    # Plot training & validation accuracy values\n",
    "    axs[0].grid(True)\n",
    "    axs[0].plot(history.history['acc'])\n",
    "    axs[0].plot(history.history['val_acc'])\n",
    "    axs[0].set(title='Model accuracy', ylabel='Accuracy', xlabel='Epoch')\n",
    "    axs[0].legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    axs[1].grid(True)\n",
    "    axs[1].plot(history.history['loss'])\n",
    "    axs[1].plot(history.history['val_loss'])\n",
    "    axs[1].set(title='Model loss',ylabel='Loss', xlabel='Epoch')\n",
    "    axs[1].legend(['Train', 'Test'], loc='upper left')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def remove_label(features, labels, label=\"MCI\"):\n",
    "    labels = pd.Series(fused_labels)\n",
    "    non_samples = labels != label\n",
    "\n",
    "    stripped_features = features[non_samples]\n",
    "    stripped_labels = labels[non_samples]\n",
    "\n",
    "    return stripped_features, stripped_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLAGS\n",
    "\n",
    "DROP_MCI = True # Whether to drop MCI samples or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a transformation pipeline\n",
    "> **Recall that feature scaling only applies to training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class AttributeRemover(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Returns a copy of matrix with attributes removed\n",
    "    \"\"\"\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return # Doesn't do anything\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X.drop(columns=self.attribute_names)\n",
    "\n",
    "class OverSampler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Returns a copy of matrix with attributes removed\n",
    "    \"\"\"\n",
    "    def __init__(self, random_state=42):\n",
    "        self.smote = SMOTE(random_state=random_state)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return None\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return self.smote.fit_resample(X,y)\n",
    "\n",
    "# Not used\n",
    "train_pipeline = Pipeline([\n",
    "                    (\"smote\", OverSampler()),\n",
    "                    (\"normalizer\", StandardScaler()) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 143 entries, 0 to 142\n",
      "Columns: 300 entries, PTID to DX_bl\n",
      "dtypes: float64(149), int64(148), object(3)\n",
      "memory usage: 335.2+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PTID</th>\n",
       "      <th>scandate</th>\n",
       "      <th>ICV</th>\n",
       "      <th>G_and_S_frontomargin_SA_lh</th>\n",
       "      <th>G_and_S_frontomargin_TH_lh</th>\n",
       "      <th>G_and_S_occipital_inf_SA_lh</th>\n",
       "      <th>G_and_S_occipital_inf_TH_lh</th>\n",
       "      <th>G_and_S_paracentral_SA_lh</th>\n",
       "      <th>G_and_S_paracentral_TH_lh</th>\n",
       "      <th>G_and_S_subcentral_SA_lh</th>\n",
       "      <th>...</th>\n",
       "      <th>S_suborbital_TH_rh</th>\n",
       "      <th>S_subparietal_SA_rh</th>\n",
       "      <th>S_subparietal_TH_rh</th>\n",
       "      <th>S_temporal_inf_SA_rh</th>\n",
       "      <th>S_temporal_inf_TH_rh</th>\n",
       "      <th>S_temporal_sup_SA_rh</th>\n",
       "      <th>S_temporal_sup_TH_rh</th>\n",
       "      <th>S_temporal_transverse_SA_rh</th>\n",
       "      <th>S_temporal_transverse_TH_rh</th>\n",
       "      <th>DX_bl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>094_S_2216</td>\n",
       "      <td>2011-05-04 08:35:04.461</td>\n",
       "      <td>307244.6</td>\n",
       "      <td>936</td>\n",
       "      <td>1.984</td>\n",
       "      <td>1158</td>\n",
       "      <td>2.107</td>\n",
       "      <td>993</td>\n",
       "      <td>2.306</td>\n",
       "      <td>1226</td>\n",
       "      <td>...</td>\n",
       "      <td>1.421</td>\n",
       "      <td>1007</td>\n",
       "      <td>1.937</td>\n",
       "      <td>763</td>\n",
       "      <td>1.759</td>\n",
       "      <td>4349</td>\n",
       "      <td>2.025</td>\n",
       "      <td>249</td>\n",
       "      <td>1.579</td>\n",
       "      <td>EMCI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>029_S_2376</td>\n",
       "      <td>2011-07-05 18:17:58.518</td>\n",
       "      <td>303135.8</td>\n",
       "      <td>855</td>\n",
       "      <td>2.160</td>\n",
       "      <td>1291</td>\n",
       "      <td>2.287</td>\n",
       "      <td>1137</td>\n",
       "      <td>1.961</td>\n",
       "      <td>1451</td>\n",
       "      <td>...</td>\n",
       "      <td>2.457</td>\n",
       "      <td>1195</td>\n",
       "      <td>1.804</td>\n",
       "      <td>635</td>\n",
       "      <td>1.970</td>\n",
       "      <td>4895</td>\n",
       "      <td>2.071</td>\n",
       "      <td>344</td>\n",
       "      <td>1.642</td>\n",
       "      <td>EMCI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>098_S_4003</td>\n",
       "      <td>2016-05-04 15:44:47.525</td>\n",
       "      <td>234729.1</td>\n",
       "      <td>849</td>\n",
       "      <td>2.122</td>\n",
       "      <td>909</td>\n",
       "      <td>2.272</td>\n",
       "      <td>788</td>\n",
       "      <td>2.430</td>\n",
       "      <td>717</td>\n",
       "      <td>...</td>\n",
       "      <td>2.222</td>\n",
       "      <td>626</td>\n",
       "      <td>1.870</td>\n",
       "      <td>784</td>\n",
       "      <td>1.826</td>\n",
       "      <td>3182</td>\n",
       "      <td>2.056</td>\n",
       "      <td>195</td>\n",
       "      <td>2.179</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>021_S_2077</td>\n",
       "      <td>2014-10-21 15:26:50.834</td>\n",
       "      <td>278496.2</td>\n",
       "      <td>762</td>\n",
       "      <td>2.237</td>\n",
       "      <td>969</td>\n",
       "      <td>2.141</td>\n",
       "      <td>1153</td>\n",
       "      <td>2.130</td>\n",
       "      <td>1156</td>\n",
       "      <td>...</td>\n",
       "      <td>2.880</td>\n",
       "      <td>1127</td>\n",
       "      <td>2.245</td>\n",
       "      <td>789</td>\n",
       "      <td>1.924</td>\n",
       "      <td>4399</td>\n",
       "      <td>2.014</td>\n",
       "      <td>243</td>\n",
       "      <td>1.826</td>\n",
       "      <td>EMCI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>021_S_5099</td>\n",
       "      <td>2013-06-11 14:47:47.885</td>\n",
       "      <td>221848.6</td>\n",
       "      <td>752</td>\n",
       "      <td>2.073</td>\n",
       "      <td>960</td>\n",
       "      <td>2.521</td>\n",
       "      <td>873</td>\n",
       "      <td>2.374</td>\n",
       "      <td>810</td>\n",
       "      <td>...</td>\n",
       "      <td>3.412</td>\n",
       "      <td>790</td>\n",
       "      <td>2.276</td>\n",
       "      <td>665</td>\n",
       "      <td>2.535</td>\n",
       "      <td>2914</td>\n",
       "      <td>2.168</td>\n",
       "      <td>183</td>\n",
       "      <td>2.166</td>\n",
       "      <td>EMCI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         PTID                 scandate       ICV  G_and_S_frontomargin_SA_lh  \\\n",
       "0  094_S_2216  2011-05-04 08:35:04.461  307244.6                         936   \n",
       "1  029_S_2376  2011-07-05 18:17:58.518  303135.8                         855   \n",
       "2  098_S_4003  2016-05-04 15:44:47.525  234729.1                         849   \n",
       "3  021_S_2077  2014-10-21 15:26:50.834  278496.2                         762   \n",
       "4  021_S_5099  2013-06-11 14:47:47.885  221848.6                         752   \n",
       "\n",
       "   G_and_S_frontomargin_TH_lh  G_and_S_occipital_inf_SA_lh  \\\n",
       "0                       1.984                         1158   \n",
       "1                       2.160                         1291   \n",
       "2                       2.122                          909   \n",
       "3                       2.237                          969   \n",
       "4                       2.073                          960   \n",
       "\n",
       "   G_and_S_occipital_inf_TH_lh  G_and_S_paracentral_SA_lh  \\\n",
       "0                        2.107                        993   \n",
       "1                        2.287                       1137   \n",
       "2                        2.272                        788   \n",
       "3                        2.141                       1153   \n",
       "4                        2.521                        873   \n",
       "\n",
       "   G_and_S_paracentral_TH_lh  G_and_S_subcentral_SA_lh  ...  \\\n",
       "0                      2.306                      1226  ...   \n",
       "1                      1.961                      1451  ...   \n",
       "2                      2.430                       717  ...   \n",
       "3                      2.130                      1156  ...   \n",
       "4                      2.374                       810  ...   \n",
       "\n",
       "   S_suborbital_TH_rh  S_subparietal_SA_rh  S_subparietal_TH_rh  \\\n",
       "0               1.421                 1007                1.937   \n",
       "1               2.457                 1195                1.804   \n",
       "2               2.222                  626                1.870   \n",
       "3               2.880                 1127                2.245   \n",
       "4               3.412                  790                2.276   \n",
       "\n",
       "   S_temporal_inf_SA_rh  S_temporal_inf_TH_rh  S_temporal_sup_SA_rh  \\\n",
       "0                   763                 1.759                  4349   \n",
       "1                   635                 1.970                  4895   \n",
       "2                   784                 1.826                  3182   \n",
       "3                   789                 1.924                  4399   \n",
       "4                   665                 2.535                  2914   \n",
       "\n",
       "   S_temporal_sup_TH_rh  S_temporal_transverse_SA_rh  \\\n",
       "0                 2.025                          249   \n",
       "1                 2.071                          344   \n",
       "2                 2.056                          195   \n",
       "3                 2.014                          243   \n",
       "4                 2.168                          183   \n",
       "\n",
       "   S_temporal_transverse_TH_rh  DX_bl  \n",
       "0                        1.579   EMCI  \n",
       "1                        1.642   EMCI  \n",
       "2                        2.179     CN  \n",
       "3                        1.826   EMCI  \n",
       "4                        2.166   EMCI  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"ICV_ADNI.csv\"\n",
    "raw_data = pd.read_csv(filename)\n",
    "print(raw_data.info())\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G_and_S_frontomargin_SA_lh</th>\n",
       "      <th>G_and_S_frontomargin_TH_lh</th>\n",
       "      <th>G_and_S_occipital_inf_SA_lh</th>\n",
       "      <th>G_and_S_occipital_inf_TH_lh</th>\n",
       "      <th>G_and_S_paracentral_SA_lh</th>\n",
       "      <th>G_and_S_paracentral_TH_lh</th>\n",
       "      <th>G_and_S_subcentral_SA_lh</th>\n",
       "      <th>G_and_S_subcentral_TH_lh</th>\n",
       "      <th>G_and_S_transv_frontopol_SA_lh</th>\n",
       "      <th>G_and_S_transv_frontopol_TH_lh</th>\n",
       "      <th>...</th>\n",
       "      <th>S_suborbital_SA_rh</th>\n",
       "      <th>S_suborbital_TH_rh</th>\n",
       "      <th>S_subparietal_SA_rh</th>\n",
       "      <th>S_subparietal_TH_rh</th>\n",
       "      <th>S_temporal_inf_SA_rh</th>\n",
       "      <th>S_temporal_inf_TH_rh</th>\n",
       "      <th>S_temporal_sup_SA_rh</th>\n",
       "      <th>S_temporal_sup_TH_rh</th>\n",
       "      <th>S_temporal_transverse_SA_rh</th>\n",
       "      <th>S_temporal_transverse_TH_rh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>936</td>\n",
       "      <td>1.984</td>\n",
       "      <td>1158</td>\n",
       "      <td>2.107</td>\n",
       "      <td>993</td>\n",
       "      <td>2.306</td>\n",
       "      <td>1226</td>\n",
       "      <td>2.359</td>\n",
       "      <td>443</td>\n",
       "      <td>2.338</td>\n",
       "      <td>...</td>\n",
       "      <td>254</td>\n",
       "      <td>1.421</td>\n",
       "      <td>1007</td>\n",
       "      <td>1.937</td>\n",
       "      <td>763</td>\n",
       "      <td>1.759</td>\n",
       "      <td>4349</td>\n",
       "      <td>2.025</td>\n",
       "      <td>249</td>\n",
       "      <td>1.579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>855</td>\n",
       "      <td>2.160</td>\n",
       "      <td>1291</td>\n",
       "      <td>2.287</td>\n",
       "      <td>1137</td>\n",
       "      <td>1.961</td>\n",
       "      <td>1451</td>\n",
       "      <td>2.059</td>\n",
       "      <td>685</td>\n",
       "      <td>2.207</td>\n",
       "      <td>...</td>\n",
       "      <td>309</td>\n",
       "      <td>2.457</td>\n",
       "      <td>1195</td>\n",
       "      <td>1.804</td>\n",
       "      <td>635</td>\n",
       "      <td>1.970</td>\n",
       "      <td>4895</td>\n",
       "      <td>2.071</td>\n",
       "      <td>344</td>\n",
       "      <td>1.642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>849</td>\n",
       "      <td>2.122</td>\n",
       "      <td>909</td>\n",
       "      <td>2.272</td>\n",
       "      <td>788</td>\n",
       "      <td>2.430</td>\n",
       "      <td>717</td>\n",
       "      <td>2.606</td>\n",
       "      <td>474</td>\n",
       "      <td>2.456</td>\n",
       "      <td>...</td>\n",
       "      <td>250</td>\n",
       "      <td>2.222</td>\n",
       "      <td>626</td>\n",
       "      <td>1.870</td>\n",
       "      <td>784</td>\n",
       "      <td>1.826</td>\n",
       "      <td>3182</td>\n",
       "      <td>2.056</td>\n",
       "      <td>195</td>\n",
       "      <td>2.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>762</td>\n",
       "      <td>2.237</td>\n",
       "      <td>969</td>\n",
       "      <td>2.141</td>\n",
       "      <td>1153</td>\n",
       "      <td>2.130</td>\n",
       "      <td>1156</td>\n",
       "      <td>2.135</td>\n",
       "      <td>421</td>\n",
       "      <td>2.282</td>\n",
       "      <td>...</td>\n",
       "      <td>180</td>\n",
       "      <td>2.880</td>\n",
       "      <td>1127</td>\n",
       "      <td>2.245</td>\n",
       "      <td>789</td>\n",
       "      <td>1.924</td>\n",
       "      <td>4399</td>\n",
       "      <td>2.014</td>\n",
       "      <td>243</td>\n",
       "      <td>1.826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>752</td>\n",
       "      <td>2.073</td>\n",
       "      <td>960</td>\n",
       "      <td>2.521</td>\n",
       "      <td>873</td>\n",
       "      <td>2.374</td>\n",
       "      <td>810</td>\n",
       "      <td>2.481</td>\n",
       "      <td>460</td>\n",
       "      <td>2.331</td>\n",
       "      <td>...</td>\n",
       "      <td>197</td>\n",
       "      <td>3.412</td>\n",
       "      <td>790</td>\n",
       "      <td>2.276</td>\n",
       "      <td>665</td>\n",
       "      <td>2.535</td>\n",
       "      <td>2914</td>\n",
       "      <td>2.168</td>\n",
       "      <td>183</td>\n",
       "      <td>2.166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 296 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   G_and_S_frontomargin_SA_lh  G_and_S_frontomargin_TH_lh  \\\n",
       "0                         936                       1.984   \n",
       "1                         855                       2.160   \n",
       "2                         849                       2.122   \n",
       "3                         762                       2.237   \n",
       "4                         752                       2.073   \n",
       "\n",
       "   G_and_S_occipital_inf_SA_lh  G_and_S_occipital_inf_TH_lh  \\\n",
       "0                         1158                        2.107   \n",
       "1                         1291                        2.287   \n",
       "2                          909                        2.272   \n",
       "3                          969                        2.141   \n",
       "4                          960                        2.521   \n",
       "\n",
       "   G_and_S_paracentral_SA_lh  G_and_S_paracentral_TH_lh  \\\n",
       "0                        993                      2.306   \n",
       "1                       1137                      1.961   \n",
       "2                        788                      2.430   \n",
       "3                       1153                      2.130   \n",
       "4                        873                      2.374   \n",
       "\n",
       "   G_and_S_subcentral_SA_lh  G_and_S_subcentral_TH_lh  \\\n",
       "0                      1226                     2.359   \n",
       "1                      1451                     2.059   \n",
       "2                       717                     2.606   \n",
       "3                      1156                     2.135   \n",
       "4                       810                     2.481   \n",
       "\n",
       "   G_and_S_transv_frontopol_SA_lh  G_and_S_transv_frontopol_TH_lh  ...  \\\n",
       "0                             443                           2.338  ...   \n",
       "1                             685                           2.207  ...   \n",
       "2                             474                           2.456  ...   \n",
       "3                             421                           2.282  ...   \n",
       "4                             460                           2.331  ...   \n",
       "\n",
       "   S_suborbital_SA_rh  S_suborbital_TH_rh  S_subparietal_SA_rh  \\\n",
       "0                 254               1.421                 1007   \n",
       "1                 309               2.457                 1195   \n",
       "2                 250               2.222                  626   \n",
       "3                 180               2.880                 1127   \n",
       "4                 197               3.412                  790   \n",
       "\n",
       "   S_subparietal_TH_rh  S_temporal_inf_SA_rh  S_temporal_inf_TH_rh  \\\n",
       "0                1.937                   763                 1.759   \n",
       "1                1.804                   635                 1.970   \n",
       "2                1.870                   784                 1.826   \n",
       "3                2.245                   789                 1.924   \n",
       "4                2.276                   665                 2.535   \n",
       "\n",
       "   S_temporal_sup_SA_rh  S_temporal_sup_TH_rh  S_temporal_transverse_SA_rh  \\\n",
       "0                  4349                 2.025                          249   \n",
       "1                  4895                 2.071                          344   \n",
       "2                  3182                 2.056                          195   \n",
       "3                  4399                 2.014                          243   \n",
       "4                  2914                 2.168                          183   \n",
       "\n",
       "   S_temporal_transverse_TH_rh  \n",
       "0                        1.579  \n",
       "1                        1.642  \n",
       "2                        2.179  \n",
       "3                        1.826  \n",
       "4                        2.166  \n",
       "\n",
       "[5 rows x 296 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_col = \"DX_bl\"\n",
    "non_feature_cols = [\"PTID\", \"scandate\", \"ICV\",label_col]\n",
    "# features = raw_data.drop(columns=[\"PTID\", \"scandate\", \"ICV\",label_col])\n",
    "\n",
    "features = AttributeRemover(attribute_names=non_feature_cols).transform(raw_data)\n",
    "\n",
    "raw_labels = raw_data[label_col].copy()\n",
    "ICVs = raw_data[\"ICV\"].copy()\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 143 entries, 0 to 142\n",
      "Columns: 148 entries, G_and_S_frontomargin_TH_lh to S_temporal_transverse_TH_rh\n",
      "dtypes: float64(148)\n",
      "memory usage: 165.4 KB\n"
     ]
    }
   ],
   "source": [
    "# Getting all the columns related to surface area\n",
    "thickness_features = [x for x in features.columns if \"SA\" in x ]\n",
    "\n",
    "# Removing SA to reduce feature dimensions\n",
    "raw_features = features.drop(columns=thickness_features)\n",
    "\n",
    "raw_features.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### TODO: Compare LRP with and w/o ICV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalize by ICV\n",
    "# features_icv_normed = raw_features.div(np.power(ICVs, 1/3), axis = \"rows\")\n",
    "# features_icv_normed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SMOTE\n",
    "\n",
    "> Generates interpolated samples to balance training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(X,y):\n",
    "    sm = SMOTE(random_state=42)\n",
    "    \n",
    "    features, labels = sm.fit_resample(X, y)\n",
    "    \n",
    "    print(\"Original: \", X.shape)\n",
    "    print(\"After Data Augmentation: \", features.shape)\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusing all the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Size: 143\n",
      "Original:\n",
      " EMCI    0.321678\n",
      "CN      0.209790\n",
      "AD      0.181818\n",
      "LMCI    0.146853\n",
      "SMC     0.139860\n",
      "Name: DX_bl, dtype: float64\n",
      "\n",
      "Fused:\n",
      " MCI    0.468531\n",
      "CN     0.349650\n",
      "AD     0.181818\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EMCI    46\n",
       "CN      30\n",
       "AD      26\n",
       "LMCI    21\n",
       "SMC     20\n",
       "Name: DX_bl, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mapping to convert labels\n",
    "fuse_maps = {\"SMC\": \"CN\", \"EMCI\":\"MCI\", \"LMCI\":\"MCI\"}\n",
    "\n",
    "# Lambda fucntion to be used with Map func\n",
    "fuse = lambda x: fuse_maps[x] if x in fuse_maps else x\n",
    "dist = lambda x: pd.Series(x).value_counts()/len(x)\n",
    "\n",
    "fused_labels = pd.Series(list(map(fuse, raw_labels)))\n",
    "\n",
    "print(\"Sample Size:\", len(fused_labels))\n",
    "print(\"Original:\\n\", dist(raw_labels))\n",
    "print()\n",
    "print(\"Fused:\\n\", dist(fused_labels))\n",
    "pd.Series(raw_labels).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting rid of MCI samples\n",
    "> Only learning CN vs AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Size: 76\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CN    0.657895\n",
       "AD    0.342105\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, labels = remove_label(raw_features, fused_labels) if DROP_MCI else (raw_features, fused_labels)\n",
    "print(\"Sample Size:\", len(labels))\n",
    "# print(labels)\n",
    "dist(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get 1 Hot Vector representation of the *fused* categorical labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: [array(['AD', 'CN'], dtype=object)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting labels to 1-Hot Vectors\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "hot_encoder = OneHotEncoder(categories=\"auto\", sparse=False)\n",
    "hot_encoder.fit(labels.values.reshape(-1,1)) # Since the function expects an array of \"features\" per sample\n",
    "\n",
    "print(\"Categories:\", hot_encoder.categories_)\n",
    "hot_encoder.transform(labels[:5].values.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the network\n",
    "\n",
    "We will build a fully connected (slightly) deep network with no drop outs or batch normalization for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "\n",
    "def exp_decay(epoch):\n",
    "    initial_lr = 0.01\n",
    "    decay_steps = 50\n",
    "    decay_rate = 0.1\n",
    "    \n",
    "    decayed_lr =  initial_lr * np.power(decay_rate, (epoch/decay_steps))\n",
    "#     print(\"New Learning Rate:\", decayed_lr)\n",
    "    return decayed_lr\n",
    "\n",
    "def build_dnn(num_features, num_labels=3):\n",
    "#     keras.backend.clear_session()\n",
    "#     reset_graph()\n",
    "    \n",
    "    reg_scale = 0.001 # For L1 Reg\n",
    "    my_reg = regularizers.l1_l2(reg_scale) # Can change this if needed\n",
    "    \n",
    "    dnn = keras.models.Sequential()\n",
    "\n",
    "    Dense = keras.layers.Dense\n",
    "\n",
    "    # Using He initialization\n",
    "    he_init = keras.initializers.he_normal()\n",
    "    \n",
    "\n",
    "    dnn.add(Dense(units = 150, activation=\"elu\", input_dim=num_features,\n",
    "                  kernel_initializer=he_init, kernel_regularizer = my_reg))\n",
    "    dnn.add(keras.layers.Dropout(0.5))\n",
    "    dnn.add(Dense(units = 100, activation=\"elu\",\n",
    "                  kernel_initializer=he_init, kernel_regularizer = my_reg))\n",
    "    dnn.add(keras.layers.Dropout(0.5))\n",
    "    dnn.add(Dense(units=50, activation='elu',\n",
    "                  kernel_initializer=he_init, kernel_regularizer = my_reg))\n",
    "    dnn.add(keras.layers.Dropout(0.5))\n",
    "    \n",
    "    dnn.add(Dense(units=num_labels, activation=\"softmax\",\n",
    "                  kernel_initializer=he_init, kernel_regularizer = my_reg)) # 5 labels -> logits for now\n",
    "    \n",
    "    nadam = keras.optimizers.Nadam()\n",
    "    NSGD = keras.optimizers.SGD(lr=exp_decay(0),momentum=0.9,nesterov=True)\n",
    "    \n",
    "    dnn.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=NSGD,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return dnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing training inputs\n",
    "\n",
    "Does not work at all without normalization. The ranges for surface area and thickness are vastly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Work/anaconda3/envs/condaflow/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/Work/anaconda3/envs/condaflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 150)               22350     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               15100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 42,602\n",
      "Trainable params: 42,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# scaler = StandardScaler() #x-u/sd\n",
    "# features = scaler.fit_transform(features) # Note that features is no longer a dataframe\n",
    "\n",
    "NUM_FEATURES = features.shape[1]\n",
    "NUM_LABELS = len(hot_encoder.categories_[0])\n",
    "\n",
    "dnn = build_dnn(NUM_FEATURES, NUM_LABELS)\n",
    "dnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Split for Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: (60, 148)\n",
      "Test Size: (16,)\n"
     ]
    }
   ],
   "source": [
    "# Get split returns a generator\n",
    "# List comprehension is one way to evaluate a generator\n",
    "X_train, y_train, X_test, y_test = list(get_split(features, labels))[0]\n",
    "print(\"Train Size:\", X_train.shape)\n",
    "print(\"Test Size:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X, y, X_test=[], y_test=[], epochs=30, batch_size=20, verbose=1, plot=True):\n",
    "    \n",
    "    ZScaler = StandardScaler().fit(X)\n",
    "    \n",
    "    X = ZScaler.transform(X)\n",
    "    \n",
    "    X_train,y_train = OverSampler().transform(X,y) # Both are np arrays now\n",
    "\n",
    "    X_test = ZScaler.transform(X_test)\n",
    "    \n",
    "    y_train = hot_encoder.transform(y_train.reshape(-1,1))\n",
    "    y_test = hot_encoder.transform(y_test.values.reshape(-1,1))\n",
    "    \n",
    "    lr_scheduler = keras.callbacks.LearningRateScheduler(exp_decay)\n",
    "    \n",
    "    callback_list = []\n",
    "    \n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size = batch_size, validation_data=(X_test, y_test),\n",
    "                       callbacks=callback_list, verbose=verbose)\n",
    "    \n",
    "    if plot: plot_history(history)\n",
    "    \n",
    "    return history, ZScaler\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Work/anaconda3/envs/condaflow/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 78 samples, validate on 16 samples\n",
      "Epoch 1/20\n",
      "78/78 [==============================] - 0s 6ms/step - loss: 12.9756 - acc: 0.4103 - val_loss: 10.7058 - val_acc: 0.6875\n",
      "Epoch 2/20\n",
      "78/78 [==============================] - 0s 107us/step - loss: 10.9432 - acc: 0.7051 - val_loss: 10.5049 - val_acc: 0.8125\n",
      "Epoch 3/20\n",
      "78/78 [==============================] - 0s 105us/step - loss: 10.7364 - acc: 0.7051 - val_loss: 10.3075 - val_acc: 0.8750\n",
      "Epoch 4/20\n",
      "78/78 [==============================] - 0s 101us/step - loss: 10.9382 - acc: 0.6667 - val_loss: 10.1629 - val_acc: 0.8750\n",
      "Epoch 5/20\n",
      "78/78 [==============================] - 0s 93us/step - loss: 10.4898 - acc: 0.7692 - val_loss: 10.1283 - val_acc: 0.8125\n",
      "Epoch 6/20\n",
      "78/78 [==============================] - 0s 107us/step - loss: 10.1190 - acc: 0.8205 - val_loss: 9.9738 - val_acc: 0.8125\n",
      "Epoch 7/20\n",
      "78/78 [==============================] - 0s 107us/step - loss: 10.1322 - acc: 0.7564 - val_loss: 9.7698 - val_acc: 0.8125\n",
      "Epoch 8/20\n",
      "78/78 [==============================] - 0s 109us/step - loss: 9.7954 - acc: 0.7949 - val_loss: 9.5840 - val_acc: 0.8125\n",
      "Epoch 9/20\n",
      "78/78 [==============================] - 0s 105us/step - loss: 9.6055 - acc: 0.8077 - val_loss: 9.4501 - val_acc: 0.8125\n",
      "Epoch 10/20\n",
      "78/78 [==============================] - 0s 100us/step - loss: 9.4577 - acc: 0.8333 - val_loss: 9.3044 - val_acc: 0.8125\n",
      "Epoch 11/20\n",
      "78/78 [==============================] - 0s 117us/step - loss: 9.0281 - acc: 0.8718 - val_loss: 9.2218 - val_acc: 0.8125\n",
      "Epoch 12/20\n",
      "78/78 [==============================] - 0s 109us/step - loss: 8.9658 - acc: 0.8974 - val_loss: 9.1007 - val_acc: 0.8125\n",
      "Epoch 13/20\n",
      "78/78 [==============================] - 0s 119us/step - loss: 8.6561 - acc: 0.9615 - val_loss: 8.9200 - val_acc: 0.8125\n",
      "Epoch 14/20\n",
      "78/78 [==============================] - 0s 101us/step - loss: 8.5643 - acc: 0.9487 - val_loss: 8.7859 - val_acc: 0.8125\n",
      "Epoch 15/20\n",
      "78/78 [==============================] - 0s 95us/step - loss: 8.3549 - acc: 0.9359 - val_loss: 8.6495 - val_acc: 0.8125\n",
      "Epoch 16/20\n",
      "78/78 [==============================] - 0s 114us/step - loss: 8.1691 - acc: 0.9744 - val_loss: 8.5220 - val_acc: 0.8125\n",
      "Epoch 17/20\n",
      "78/78 [==============================] - 0s 99us/step - loss: 7.9885 - acc: 0.9744 - val_loss: 8.4088 - val_acc: 0.8125\n",
      "Epoch 18/20\n",
      "78/78 [==============================] - 0s 116us/step - loss: 7.8776 - acc: 0.9359 - val_loss: 8.2802 - val_acc: 0.8125\n",
      "Epoch 19/20\n",
      "78/78 [==============================] - 0s 97us/step - loss: 7.7751 - acc: 0.9231 - val_loss: 8.1371 - val_acc: 0.8125\n",
      "Epoch 20/20\n",
      "78/78 [==============================] - 0s 103us/step - loss: 7.5540 - acc: 1.0000 - val_loss: 8.0169 - val_acc: 0.8125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d83a61960334325a4eb0f4d5e86497c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dnn = build_dnn(NUM_FEATURES, NUM_LABELS)\n",
    "history, ZScaler = train_model(dnn, X_train, y_train, X_test, y_test, epochs=20, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> AD vs CN\n",
    "\n",
    "loss: 3.4295 - acc: 0.8846 - val_loss: 3.9303 - val_acc: 0.8125 - w/ ICV\n",
    "val_acc = 0.875 -w/o ICV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[5 0]\n",
      " [3 8]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8aa6bc78f54e25bd45ca7a1e0c98a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_confusion(model, X_test, y_test):\n",
    "    y_pred_probs = dnn.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    y_true = np.argmax(hot_encoder.transform(y_test.values.reshape(-1,1)), axis=1)\n",
    "    plot_confusion_matrix(y_true, y_pred, classes=hot_encoder.categories_[0])\n",
    "make_confusion(dnn, ZScaler.transform(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AD', 'CN'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hot_encoder.categories_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using K=10 Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2 19 23 34 42 49 64 71]\n",
      "Scores on test set: loss=1.000 accuracy=0.7500\n",
      "[25 33 35 36 40 47 63 74]\n",
      "Scores on test set: loss=1.000 accuracy=0.8750\n",
      "[ 3  5  6 10 17 45 58 61]\n",
      "Scores on test set: loss=1.000 accuracy=0.8750\n",
      "[ 8 13 18 21 43 67 72 73]\n",
      "Scores on test set: loss=1.000 accuracy=0.8750\n",
      "[11 14 15 22 32 44 48 57]\n",
      "Scores on test set: loss=1.000 accuracy=0.8750\n",
      "[ 0  7 38 50 54 55 56 70]\n",
      "Scores on test set: loss=1.000 accuracy=0.7500\n",
      "[ 1 16 28 41 51 60 66]\n",
      "Scores on test set: loss=1.000 accuracy=0.8571\n",
      "[ 4 29 31 53 59 65 69]\n",
      "Scores on test set: loss=1.000 accuracy=1.0000\n",
      "[12 24 27 30 37 46 75]\n",
      "Scores on test set: loss=1.000 accuracy=0.8571\n",
      "[ 9 20 26 39 52 62 68]\n",
      "Scores on test set: loss=1.000 accuracy=0.7143\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold as KFold\n",
    "keras.backend.clear_session()\n",
    "\n",
    "def getKF(X,y, n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42 ) #Default = 10\n",
    "\n",
    "    for train_index, test_index in kf.split(X,y):\n",
    "        X_train = X.iloc[train_index]\n",
    "        y_train = y.iloc[train_index]\n",
    "        X_test = X.iloc[test_index]\n",
    "        y_test = y.iloc[test_index]\n",
    "        \n",
    "        yield X_train, y_train, X_test, y_test, test_index\n",
    "\n",
    "histories = []\n",
    "testing_indxs =[]\n",
    "predictions = []\n",
    "true_labels = []\n",
    "zoo = []\n",
    "for X_train, y_train, X_test, y_test, test_index in getKF(features, labels):\n",
    "    print(test_index)\n",
    "    dnn = build_dnn(NUM_FEATURES, NUM_LABELS)\n",
    "    history, ZScaler = train_model(dnn,X_train, y_train, X_test, y_test, verbose=0, plot=False, epochs=100, batch_size=10)\n",
    "    \n",
    "    # Updating all information arrays\n",
    "    histories.append(history)\n",
    "    testing_indxs.append(test_index)\n",
    "    zoo.append(dnn)\n",
    "    \n",
    "    y_pred_probs = dnn.predict(ZScaler.transform(X_test))\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    y_true = np.argmax(hot_encoder.transform(y_test.values.reshape(-1,1)), axis=1)\n",
    "    \n",
    "    predictions.extend(y_pred)\n",
    "    true_labels.extend(y_true)\n",
    "    \n",
    "    print(\"Scores on test set: loss={:0.3f} accuracy={:.4f}\".format(history.history[\"acc\"][-1], history.history[\"val_acc\"][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[20  6]\n",
      " [ 6 44]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fb116ecad3b4c0580736c381ff6972b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12eed10b8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_confusion_matrix(predictions, true_labels, classes=hot_encoder.categories_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[0.8        0.2       ]\n",
      " [0.11764706 0.88235294]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d56c18f2d548f4831153031e80f46b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x13a8280b8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_confusion_matrix(predictions, true_labels, classes=hot_encoder.categories_[0], normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a071fc0ad04846e88c361bd71ed729f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Num is the figure number and clear tells it to clear the figure if it already exists\n",
    "# plt.close(fig)\n",
    "fig, axs = plt.subplots(num=\"KF Eval\",\n",
    "                        nrows=len(histories)//2, ncols=2,\n",
    "                        figsize=(10,10), sharex=True, sharey=True)\n",
    "axs=axs.flatten()\n",
    "dfs = []\n",
    "\n",
    "for i,history in enumerate(histories):\n",
    "    df = pd.DataFrame(history.history)\n",
    "    dfs.append(df)\n",
    "#     axs[i].grid(True)\n",
    "    df[[\"acc\",\"val_acc\"]].plot(ax=axs[i], grid=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_accs = [df[\"val_acc\"].iloc[-1] for df in dfs]\n",
    "# print(\"Average:\",np.mean(val_accs))\n",
    "# plt.bar(x=range(10),height=val_accs)\n",
    "# # plt.scatter(x=range(10), y=np.mean(val_accs))\n",
    "# plt.xlabel(\"Fold Number\")\n",
    "# plt.ylabel(\"Acc\")\n",
    "# plt.title(\"Accuracies for Each Fold\")\n",
    "# val_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets pick the best model from the folds..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 0s 104us/step\n"
     ]
    }
   ],
   "source": [
    "idx = -3\n",
    "# from keras import backend as K\n",
    "# K.clear_session()\n",
    "best_dnn = zoo[idx]\n",
    "# validation = labels.iloc[testing_indxs[-3]]\n",
    "# best_dnn.save(\"best_dnn.h5\")\n",
    "best_dnn.evaluate(features,hot_encoder.transform(labels.values.reshape(-1,1)))\n",
    "\n",
    "best_dnn.save(\"best_dnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Data Info\n",
    "full_data = features.copy()\n",
    "full_data[\"labels\"] = labels\n",
    "full_data.to_csv(\"AD_CN_TH.csv\")\n",
    "pd.DataFrame(testing_indxs[idx]).to_csv(\"test_indices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condaflow",
   "language": "python",
   "name": "condaflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
