{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "import sys\n",
    "# sys.path.append(\"/Users/Work/Developer/interpretDL/interprettensor\")\n",
    "root_logdir = \"./tf_logs\"\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib widget\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "######### Taken from sklearn #######\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=[10,10])\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "def get1hot(y_train,y_test):\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "    enc = OneHotEncoder(categories=\"auto\", sparse=False)\n",
    "    y_train_1hot = enc.fit_transform([[label] for label in y_train]) # Since the function expects an array of \"features\" per sample\n",
    "    y_test_1hot = enc.fit_transform([[label] for label in y_test])\n",
    "\n",
    "    return y_train_1hot, y_test_1hot\n",
    "\n",
    "def get_split(features, labels):\n",
    "    \n",
    "    # The train set will have equal amounts of each target class\n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    for train_index, test_index in split.split(features, labels):\n",
    "        X_train = features[train_index]\n",
    "        y_train = labels.iloc[train_index]\n",
    "        X_test = features[test_index]\n",
    "        y_test = labels.iloc[test_index]\n",
    "        \n",
    "        yield X_train, y_train, X_test, y_test\n",
    "\n",
    "def plot_history(history):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(18,6))\n",
    "    \n",
    "    # Plot training & validation accuracy values\n",
    "    axs[0].grid(True)\n",
    "    axs[0].plot(history.history['acc'])\n",
    "    axs[0].plot(history.history['val_acc'])\n",
    "    axs[0].set(title='Model accuracy', ylabel='Accuracy', xlabel='Epoch')\n",
    "    axs[0].legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    axs[1].grid(True)\n",
    "    axs[1].plot(history.history['loss'])\n",
    "    axs[1].plot(history.history['val_loss'])\n",
    "    axs[1].set(title='Model loss',ylabel='Loss', xlabel='Epoch')\n",
    "    axs[1].legend(['Train', 'Test'], loc='upper left')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def remove_label(features, labels, label=\"MCI\"):\n",
    "    labels = pd.Series(fused_labels)\n",
    "    non_samples = labels != label\n",
    "\n",
    "    stripped_features = features[non_samples]\n",
    "    stripped_labels = labels[non_samples]\n",
    "\n",
    "    return stripped_features, stripped_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLAGS\n",
    "\n",
    "DROP_MCI = True # Whether to drop MCI samples or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a transformation pipeline\n",
    "> **Recall that feature scaling only applies to training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class AttributeRemover(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Returns a copy of matrix with attributes removed\n",
    "    \"\"\"\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return # Doesn't do anything\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X.drop(columns=self.attribute_names)\n",
    "\n",
    "class OverSampler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Returns a copy of matrix with attributes removed\n",
    "    \"\"\"\n",
    "    def __init__(self, random_state=42):\n",
    "        self.smote = SMOTE(random_state=random_state)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return None\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return self.smote.fit_resample(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 143 entries, 0 to 142\n",
      "Columns: 300 entries, PTID to DX_bl\n",
      "dtypes: float64(149), int64(148), object(3)\n",
      "memory usage: 335.2+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PTID</th>\n",
       "      <th>scandate</th>\n",
       "      <th>ICV</th>\n",
       "      <th>G_and_S_frontomargin_SA_lh</th>\n",
       "      <th>G_and_S_frontomargin_TH_lh</th>\n",
       "      <th>G_and_S_occipital_inf_SA_lh</th>\n",
       "      <th>G_and_S_occipital_inf_TH_lh</th>\n",
       "      <th>G_and_S_paracentral_SA_lh</th>\n",
       "      <th>G_and_S_paracentral_TH_lh</th>\n",
       "      <th>G_and_S_subcentral_SA_lh</th>\n",
       "      <th>...</th>\n",
       "      <th>S_suborbital_TH_rh</th>\n",
       "      <th>S_subparietal_SA_rh</th>\n",
       "      <th>S_subparietal_TH_rh</th>\n",
       "      <th>S_temporal_inf_SA_rh</th>\n",
       "      <th>S_temporal_inf_TH_rh</th>\n",
       "      <th>S_temporal_sup_SA_rh</th>\n",
       "      <th>S_temporal_sup_TH_rh</th>\n",
       "      <th>S_temporal_transverse_SA_rh</th>\n",
       "      <th>S_temporal_transverse_TH_rh</th>\n",
       "      <th>DX_bl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>094_S_2216</td>\n",
       "      <td>2011-05-04 08:35:04.461</td>\n",
       "      <td>307244.6</td>\n",
       "      <td>936</td>\n",
       "      <td>1.984</td>\n",
       "      <td>1158</td>\n",
       "      <td>2.107</td>\n",
       "      <td>993</td>\n",
       "      <td>2.306</td>\n",
       "      <td>1226</td>\n",
       "      <td>...</td>\n",
       "      <td>1.421</td>\n",
       "      <td>1007</td>\n",
       "      <td>1.937</td>\n",
       "      <td>763</td>\n",
       "      <td>1.759</td>\n",
       "      <td>4349</td>\n",
       "      <td>2.025</td>\n",
       "      <td>249</td>\n",
       "      <td>1.579</td>\n",
       "      <td>EMCI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>029_S_2376</td>\n",
       "      <td>2011-07-05 18:17:58.518</td>\n",
       "      <td>303135.8</td>\n",
       "      <td>855</td>\n",
       "      <td>2.160</td>\n",
       "      <td>1291</td>\n",
       "      <td>2.287</td>\n",
       "      <td>1137</td>\n",
       "      <td>1.961</td>\n",
       "      <td>1451</td>\n",
       "      <td>...</td>\n",
       "      <td>2.457</td>\n",
       "      <td>1195</td>\n",
       "      <td>1.804</td>\n",
       "      <td>635</td>\n",
       "      <td>1.970</td>\n",
       "      <td>4895</td>\n",
       "      <td>2.071</td>\n",
       "      <td>344</td>\n",
       "      <td>1.642</td>\n",
       "      <td>EMCI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>098_S_4003</td>\n",
       "      <td>2016-05-04 15:44:47.525</td>\n",
       "      <td>234729.1</td>\n",
       "      <td>849</td>\n",
       "      <td>2.122</td>\n",
       "      <td>909</td>\n",
       "      <td>2.272</td>\n",
       "      <td>788</td>\n",
       "      <td>2.430</td>\n",
       "      <td>717</td>\n",
       "      <td>...</td>\n",
       "      <td>2.222</td>\n",
       "      <td>626</td>\n",
       "      <td>1.870</td>\n",
       "      <td>784</td>\n",
       "      <td>1.826</td>\n",
       "      <td>3182</td>\n",
       "      <td>2.056</td>\n",
       "      <td>195</td>\n",
       "      <td>2.179</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>021_S_2077</td>\n",
       "      <td>2014-10-21 15:26:50.834</td>\n",
       "      <td>278496.2</td>\n",
       "      <td>762</td>\n",
       "      <td>2.237</td>\n",
       "      <td>969</td>\n",
       "      <td>2.141</td>\n",
       "      <td>1153</td>\n",
       "      <td>2.130</td>\n",
       "      <td>1156</td>\n",
       "      <td>...</td>\n",
       "      <td>2.880</td>\n",
       "      <td>1127</td>\n",
       "      <td>2.245</td>\n",
       "      <td>789</td>\n",
       "      <td>1.924</td>\n",
       "      <td>4399</td>\n",
       "      <td>2.014</td>\n",
       "      <td>243</td>\n",
       "      <td>1.826</td>\n",
       "      <td>EMCI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>021_S_5099</td>\n",
       "      <td>2013-06-11 14:47:47.885</td>\n",
       "      <td>221848.6</td>\n",
       "      <td>752</td>\n",
       "      <td>2.073</td>\n",
       "      <td>960</td>\n",
       "      <td>2.521</td>\n",
       "      <td>873</td>\n",
       "      <td>2.374</td>\n",
       "      <td>810</td>\n",
       "      <td>...</td>\n",
       "      <td>3.412</td>\n",
       "      <td>790</td>\n",
       "      <td>2.276</td>\n",
       "      <td>665</td>\n",
       "      <td>2.535</td>\n",
       "      <td>2914</td>\n",
       "      <td>2.168</td>\n",
       "      <td>183</td>\n",
       "      <td>2.166</td>\n",
       "      <td>EMCI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         PTID                 scandate       ICV  G_and_S_frontomargin_SA_lh  \\\n",
       "0  094_S_2216  2011-05-04 08:35:04.461  307244.6                         936   \n",
       "1  029_S_2376  2011-07-05 18:17:58.518  303135.8                         855   \n",
       "2  098_S_4003  2016-05-04 15:44:47.525  234729.1                         849   \n",
       "3  021_S_2077  2014-10-21 15:26:50.834  278496.2                         762   \n",
       "4  021_S_5099  2013-06-11 14:47:47.885  221848.6                         752   \n",
       "\n",
       "   G_and_S_frontomargin_TH_lh  G_and_S_occipital_inf_SA_lh  \\\n",
       "0                       1.984                         1158   \n",
       "1                       2.160                         1291   \n",
       "2                       2.122                          909   \n",
       "3                       2.237                          969   \n",
       "4                       2.073                          960   \n",
       "\n",
       "   G_and_S_occipital_inf_TH_lh  G_and_S_paracentral_SA_lh  \\\n",
       "0                        2.107                        993   \n",
       "1                        2.287                       1137   \n",
       "2                        2.272                        788   \n",
       "3                        2.141                       1153   \n",
       "4                        2.521                        873   \n",
       "\n",
       "   G_and_S_paracentral_TH_lh  G_and_S_subcentral_SA_lh  ...  \\\n",
       "0                      2.306                      1226  ...   \n",
       "1                      1.961                      1451  ...   \n",
       "2                      2.430                       717  ...   \n",
       "3                      2.130                      1156  ...   \n",
       "4                      2.374                       810  ...   \n",
       "\n",
       "   S_suborbital_TH_rh  S_subparietal_SA_rh  S_subparietal_TH_rh  \\\n",
       "0               1.421                 1007                1.937   \n",
       "1               2.457                 1195                1.804   \n",
       "2               2.222                  626                1.870   \n",
       "3               2.880                 1127                2.245   \n",
       "4               3.412                  790                2.276   \n",
       "\n",
       "   S_temporal_inf_SA_rh  S_temporal_inf_TH_rh  S_temporal_sup_SA_rh  \\\n",
       "0                   763                 1.759                  4349   \n",
       "1                   635                 1.970                  4895   \n",
       "2                   784                 1.826                  3182   \n",
       "3                   789                 1.924                  4399   \n",
       "4                   665                 2.535                  2914   \n",
       "\n",
       "   S_temporal_sup_TH_rh  S_temporal_transverse_SA_rh  \\\n",
       "0                 2.025                          249   \n",
       "1                 2.071                          344   \n",
       "2                 2.056                          195   \n",
       "3                 2.014                          243   \n",
       "4                 2.168                          183   \n",
       "\n",
       "   S_temporal_transverse_TH_rh  DX_bl  \n",
       "0                        1.579   EMCI  \n",
       "1                        1.642   EMCI  \n",
       "2                        2.179     CN  \n",
       "3                        1.826   EMCI  \n",
       "4                        2.166   EMCI  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"ICV_ADNI.csv\"\n",
    "raw_data = pd.read_csv(filename)\n",
    "print(raw_data.info())\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G_and_S_frontomargin_SA_lh</th>\n",
       "      <th>G_and_S_frontomargin_TH_lh</th>\n",
       "      <th>G_and_S_occipital_inf_SA_lh</th>\n",
       "      <th>G_and_S_occipital_inf_TH_lh</th>\n",
       "      <th>G_and_S_paracentral_SA_lh</th>\n",
       "      <th>G_and_S_paracentral_TH_lh</th>\n",
       "      <th>G_and_S_subcentral_SA_lh</th>\n",
       "      <th>G_and_S_subcentral_TH_lh</th>\n",
       "      <th>G_and_S_transv_frontopol_SA_lh</th>\n",
       "      <th>G_and_S_transv_frontopol_TH_lh</th>\n",
       "      <th>...</th>\n",
       "      <th>S_suborbital_SA_rh</th>\n",
       "      <th>S_suborbital_TH_rh</th>\n",
       "      <th>S_subparietal_SA_rh</th>\n",
       "      <th>S_subparietal_TH_rh</th>\n",
       "      <th>S_temporal_inf_SA_rh</th>\n",
       "      <th>S_temporal_inf_TH_rh</th>\n",
       "      <th>S_temporal_sup_SA_rh</th>\n",
       "      <th>S_temporal_sup_TH_rh</th>\n",
       "      <th>S_temporal_transverse_SA_rh</th>\n",
       "      <th>S_temporal_transverse_TH_rh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>936</td>\n",
       "      <td>1.984</td>\n",
       "      <td>1158</td>\n",
       "      <td>2.107</td>\n",
       "      <td>993</td>\n",
       "      <td>2.306</td>\n",
       "      <td>1226</td>\n",
       "      <td>2.359</td>\n",
       "      <td>443</td>\n",
       "      <td>2.338</td>\n",
       "      <td>...</td>\n",
       "      <td>254</td>\n",
       "      <td>1.421</td>\n",
       "      <td>1007</td>\n",
       "      <td>1.937</td>\n",
       "      <td>763</td>\n",
       "      <td>1.759</td>\n",
       "      <td>4349</td>\n",
       "      <td>2.025</td>\n",
       "      <td>249</td>\n",
       "      <td>1.579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>855</td>\n",
       "      <td>2.160</td>\n",
       "      <td>1291</td>\n",
       "      <td>2.287</td>\n",
       "      <td>1137</td>\n",
       "      <td>1.961</td>\n",
       "      <td>1451</td>\n",
       "      <td>2.059</td>\n",
       "      <td>685</td>\n",
       "      <td>2.207</td>\n",
       "      <td>...</td>\n",
       "      <td>309</td>\n",
       "      <td>2.457</td>\n",
       "      <td>1195</td>\n",
       "      <td>1.804</td>\n",
       "      <td>635</td>\n",
       "      <td>1.970</td>\n",
       "      <td>4895</td>\n",
       "      <td>2.071</td>\n",
       "      <td>344</td>\n",
       "      <td>1.642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>849</td>\n",
       "      <td>2.122</td>\n",
       "      <td>909</td>\n",
       "      <td>2.272</td>\n",
       "      <td>788</td>\n",
       "      <td>2.430</td>\n",
       "      <td>717</td>\n",
       "      <td>2.606</td>\n",
       "      <td>474</td>\n",
       "      <td>2.456</td>\n",
       "      <td>...</td>\n",
       "      <td>250</td>\n",
       "      <td>2.222</td>\n",
       "      <td>626</td>\n",
       "      <td>1.870</td>\n",
       "      <td>784</td>\n",
       "      <td>1.826</td>\n",
       "      <td>3182</td>\n",
       "      <td>2.056</td>\n",
       "      <td>195</td>\n",
       "      <td>2.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>762</td>\n",
       "      <td>2.237</td>\n",
       "      <td>969</td>\n",
       "      <td>2.141</td>\n",
       "      <td>1153</td>\n",
       "      <td>2.130</td>\n",
       "      <td>1156</td>\n",
       "      <td>2.135</td>\n",
       "      <td>421</td>\n",
       "      <td>2.282</td>\n",
       "      <td>...</td>\n",
       "      <td>180</td>\n",
       "      <td>2.880</td>\n",
       "      <td>1127</td>\n",
       "      <td>2.245</td>\n",
       "      <td>789</td>\n",
       "      <td>1.924</td>\n",
       "      <td>4399</td>\n",
       "      <td>2.014</td>\n",
       "      <td>243</td>\n",
       "      <td>1.826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>752</td>\n",
       "      <td>2.073</td>\n",
       "      <td>960</td>\n",
       "      <td>2.521</td>\n",
       "      <td>873</td>\n",
       "      <td>2.374</td>\n",
       "      <td>810</td>\n",
       "      <td>2.481</td>\n",
       "      <td>460</td>\n",
       "      <td>2.331</td>\n",
       "      <td>...</td>\n",
       "      <td>197</td>\n",
       "      <td>3.412</td>\n",
       "      <td>790</td>\n",
       "      <td>2.276</td>\n",
       "      <td>665</td>\n",
       "      <td>2.535</td>\n",
       "      <td>2914</td>\n",
       "      <td>2.168</td>\n",
       "      <td>183</td>\n",
       "      <td>2.166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 296 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   G_and_S_frontomargin_SA_lh  G_and_S_frontomargin_TH_lh  \\\n",
       "0                         936                       1.984   \n",
       "1                         855                       2.160   \n",
       "2                         849                       2.122   \n",
       "3                         762                       2.237   \n",
       "4                         752                       2.073   \n",
       "\n",
       "   G_and_S_occipital_inf_SA_lh  G_and_S_occipital_inf_TH_lh  \\\n",
       "0                         1158                        2.107   \n",
       "1                         1291                        2.287   \n",
       "2                          909                        2.272   \n",
       "3                          969                        2.141   \n",
       "4                          960                        2.521   \n",
       "\n",
       "   G_and_S_paracentral_SA_lh  G_and_S_paracentral_TH_lh  \\\n",
       "0                        993                      2.306   \n",
       "1                       1137                      1.961   \n",
       "2                        788                      2.430   \n",
       "3                       1153                      2.130   \n",
       "4                        873                      2.374   \n",
       "\n",
       "   G_and_S_subcentral_SA_lh  G_and_S_subcentral_TH_lh  \\\n",
       "0                      1226                     2.359   \n",
       "1                      1451                     2.059   \n",
       "2                       717                     2.606   \n",
       "3                      1156                     2.135   \n",
       "4                       810                     2.481   \n",
       "\n",
       "   G_and_S_transv_frontopol_SA_lh  G_and_S_transv_frontopol_TH_lh  ...  \\\n",
       "0                             443                           2.338  ...   \n",
       "1                             685                           2.207  ...   \n",
       "2                             474                           2.456  ...   \n",
       "3                             421                           2.282  ...   \n",
       "4                             460                           2.331  ...   \n",
       "\n",
       "   S_suborbital_SA_rh  S_suborbital_TH_rh  S_subparietal_SA_rh  \\\n",
       "0                 254               1.421                 1007   \n",
       "1                 309               2.457                 1195   \n",
       "2                 250               2.222                  626   \n",
       "3                 180               2.880                 1127   \n",
       "4                 197               3.412                  790   \n",
       "\n",
       "   S_subparietal_TH_rh  S_temporal_inf_SA_rh  S_temporal_inf_TH_rh  \\\n",
       "0                1.937                   763                 1.759   \n",
       "1                1.804                   635                 1.970   \n",
       "2                1.870                   784                 1.826   \n",
       "3                2.245                   789                 1.924   \n",
       "4                2.276                   665                 2.535   \n",
       "\n",
       "   S_temporal_sup_SA_rh  S_temporal_sup_TH_rh  S_temporal_transverse_SA_rh  \\\n",
       "0                  4349                 2.025                          249   \n",
       "1                  4895                 2.071                          344   \n",
       "2                  3182                 2.056                          195   \n",
       "3                  4399                 2.014                          243   \n",
       "4                  2914                 2.168                          183   \n",
       "\n",
       "   S_temporal_transverse_TH_rh  \n",
       "0                        1.579  \n",
       "1                        1.642  \n",
       "2                        2.179  \n",
       "3                        1.826  \n",
       "4                        2.166  \n",
       "\n",
       "[5 rows x 296 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_col = \"DX_bl\"\n",
    "non_feature_cols = [\"PTID\", \"scandate\", \"ICV\",label_col]\n",
    "# features = raw_data.drop(columns=[\"PTID\", \"scandate\", \"ICV\",label_col])\n",
    "\n",
    "features = AttributeRemover(attribute_names=non_feature_cols).transform(raw_data)\n",
    "\n",
    "raw_labels = raw_data[label_col].copy()\n",
    "ICVs = raw_data[\"ICV\"].copy()\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 143 entries, 0 to 142\n",
      "Columns: 148 entries, G_and_S_frontomargin_TH_lh to S_temporal_transverse_TH_rh\n",
      "dtypes: float64(148)\n",
      "memory usage: 165.4 KB\n"
     ]
    }
   ],
   "source": [
    "# Getting all the columns related to surface area\n",
    "thickness_features = [x for x in features.columns if \"SA\" in x ]\n",
    "\n",
    "# Removing SA to reduce feature dimensions\n",
    "raw_features = features.drop(columns=thickness_features)\n",
    "\n",
    "raw_features.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### TODO: Compare LRP with and w/o ICV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalize by ICV\n",
    "# features_icv_normed = raw_features.div(np.power(ICVs, 1/3), axis = \"rows\")\n",
    "# features_icv_normed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SMOTE\n",
    "\n",
    "> Generates interpolated samples to balance training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(X,y):\n",
    "    sm = SMOTE(random_state=42)\n",
    "    \n",
    "    features, labels = sm.fit_resample(X, y)\n",
    "    \n",
    "    print(\"Original: \", X.shape)\n",
    "    print(\"After Data Augmentation: \", features.shape)\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusing all the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Size: 143\n",
      "Original:\n",
      " EMCI    0.321678\n",
      "CN      0.209790\n",
      "AD      0.181818\n",
      "LMCI    0.146853\n",
      "SMC     0.139860\n",
      "Name: DX_bl, dtype: float64\n",
      "\n",
      "Fused:\n",
      " MCI    0.468531\n",
      "CN     0.349650\n",
      "AD     0.181818\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EMCI    46\n",
       "CN      30\n",
       "AD      26\n",
       "LMCI    21\n",
       "SMC     20\n",
       "Name: DX_bl, dtype: int64"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mapping to convert labels\n",
    "fuse_maps = {\"SMC\": \"CN\", \"EMCI\":\"MCI\", \"LMCI\":\"MCI\"}\n",
    "\n",
    "# Lambda fucntion to be used with Map func\n",
    "fuse = lambda x: fuse_maps[x] if x in fuse_maps else x\n",
    "dist = lambda x: pd.Series(x).value_counts()/len(x)\n",
    "\n",
    "fused_labels = pd.Series(list(map(fuse, raw_labels)))\n",
    "\n",
    "print(\"Sample Size:\", len(fused_labels))\n",
    "print(\"Original:\\n\", dist(raw_labels))\n",
    "print()\n",
    "print(\"Fused:\\n\", dist(fused_labels))\n",
    "pd.Series(raw_labels).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting rid of MCI samples\n",
    "> Only learning CN vs AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Size: 76\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CN    0.657895\n",
       "AD    0.342105\n",
       "dtype: float64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, labels = remove_label(raw_features, fused_labels) if DROP_MCI else (raw_features, fused_labels)\n",
    "print(\"Sample Size:\", len(labels))\n",
    "# print(labels)\n",
    "dist(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get 1 Hot Vector representation of the *fused* categorical labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: [array(['AD', 'CN'], dtype=object)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting labels to 1-Hot Vectors\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "hot_encoder = OneHotEncoder(categories=\"auto\", sparse=False)\n",
    "hot_encoder.fit(labels.values.reshape(-1,1)) # Since the function expects an array of \"features\" per sample\n",
    "\n",
    "print(\"Categories:\", hot_encoder.categories_)\n",
    "hot_encoder.transform(labels[:5].values.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing training inputs\n",
    "\n",
    "Does not work at all without normalization. The ranges for surface area and thickness are vastly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler = StandardScaler() #x-u/sd\n",
    "# features = scaler.fit_transform(features) # Note that features is no longer a dataframe\n",
    "\n",
    "NUM_FEATURES = features.shape[1]\n",
    "NUM_LABELS = len(hot_encoder.categories_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Split for Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: (60, 148)\n",
      "Test Size: (16,)\n"
     ]
    }
   ],
   "source": [
    "# Get split returns a generator\n",
    "# List comprehension is one way to evaluate a generator\n",
    "X_train, y_train, X_test, y_test = list(get_split(features, labels))[0]\n",
    "print(\"Train Size:\", X_train.shape)\n",
    "print(\"Test Size:\", y_test.shape)\n",
    "# y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the network\n",
    "\n",
    "We will build a fully connected (slightly) deep network with no drop outs or batch normalization for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 150)               22350     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               15100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 42,602\n",
      "Trainable params: 42,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "\n",
    "def exp_decay(epoch):\n",
    "    initial_lr = 0.01\n",
    "    decay_steps = 50\n",
    "    decay_rate = 0.1\n",
    "    \n",
    "    decayed_lr =  initial_lr * np.power(decay_rate, (epoch/decay_steps))\n",
    "#     print(\"New Learning Rate:\", decayed_lr)\n",
    "    return decayed_lr\n",
    "\n",
    "def build_dnn(num_features, num_labels=3):\n",
    "    keras.backend.clear_session()\n",
    "    reset_graph()\n",
    "    \n",
    "    reg_scale = 0.001 # For L1 Reg\n",
    "    my_reg = regularizers.l1_l2(reg_scale) # Can change this if needed\n",
    "    \n",
    "    dnn = keras.models.Sequential()\n",
    "\n",
    "    Dense = keras.layers.Dense\n",
    "\n",
    "    # Using He initialization\n",
    "    he_init = keras.initializers.he_normal()\n",
    "    \n",
    "\n",
    "    dnn.add(Dense(units = 150, activation=\"elu\", input_dim=num_features,\n",
    "                  kernel_initializer=he_init, kernel_regularizer = my_reg))\n",
    "    dnn.add(keras.layers.Dropout(0.5))\n",
    "    dnn.add(Dense(units = 100, activation=\"elu\",\n",
    "                  kernel_initializer=he_init, kernel_regularizer = my_reg))\n",
    "    dnn.add(keras.layers.Dropout(0.5))\n",
    "    dnn.add(Dense(units=50, activation='elu',\n",
    "                  kernel_initializer=he_init, kernel_regularizer = my_reg))\n",
    "    dnn.add(keras.layers.Dropout(0.5))\n",
    "    \n",
    "    dnn.add(Dense(units=num_labels, activation=\"softmax\",\n",
    "                  kernel_initializer=he_init, kernel_regularizer = my_reg)) # 5 labels -> logits for now\n",
    "    \n",
    "    nadam = keras.optimizers.Nadam()\n",
    "    NSGD = keras.optimizers.SGD(lr=exp_decay(0),momentum=0.9,nesterov=True)\n",
    "    \n",
    "    dnn.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=NSGD,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return dnn\n",
    "\n",
    "dnn = build_dnn(NUM_FEATURES, NUM_LABELS)\n",
    "dnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X, y, X_test=[], y_test=[], epochs=30, batch_size=20, verbose=1, plot=True):\n",
    "    \n",
    "    X_train,y_train = balance_data(X,y) # Both are np arrays now\n",
    "    \n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    \n",
    "    y_train = hot_encoder.transform(y_train.reshape(-1,1))\n",
    "    y_test = hot_encoder.transform(y_test.values.reshape(-1,1))\n",
    "    \n",
    "    lr_scheduler = keras.callbacks.LearningRateScheduler(exp_decay)\n",
    "    \n",
    "    callback_list = []\n",
    "    \n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size = batch_size, validation_data=(X_test, y_test),\n",
    "                       callbacks=callback_list, verbose=verbose)\n",
    "    \n",
    "    if plot: plot_history(history)\n",
    "    \n",
    "    return history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  (60, 148)\n",
      "After Data Augmentation:  (78, 148)\n",
      "Train on 78 samples, validate on 16 samples\n",
      "Epoch 1/20\n",
      "78/78 [==============================] - 1s 7ms/step - loss: 11.6805 - acc: 0.5897 - val_loss: 10.7933 - val_acc: 0.6875\n",
      "Epoch 2/20\n",
      "78/78 [==============================] - 0s 184us/step - loss: 11.2255 - acc: 0.7051 - val_loss: 11.0492 - val_acc: 0.8125\n",
      "Epoch 3/20\n",
      "78/78 [==============================] - 0s 174us/step - loss: 10.9869 - acc: 0.7692 - val_loss: 10.8099 - val_acc: 0.8750\n",
      "Epoch 4/20\n",
      "78/78 [==============================] - 0s 164us/step - loss: 10.8288 - acc: 0.8205 - val_loss: 10.5460 - val_acc: 0.8750\n",
      "Epoch 5/20\n",
      "78/78 [==============================] - 0s 191us/step - loss: 9.9648 - acc: 0.8462 - val_loss: 11.1995 - val_acc: 0.8750\n",
      "Epoch 6/20\n",
      "78/78 [==============================] - 0s 169us/step - loss: 9.5050 - acc: 0.9231 - val_loss: 11.0076 - val_acc: 0.8750\n",
      "Epoch 7/20\n",
      "78/78 [==============================] - 0s 193us/step - loss: 9.2554 - acc: 0.9231 - val_loss: 10.7859 - val_acc: 0.8750\n",
      "Epoch 8/20\n",
      "78/78 [==============================] - 0s 175us/step - loss: 9.3271 - acc: 0.8974 - val_loss: 10.5595 - val_acc: 0.8750\n",
      "Epoch 9/20\n",
      "78/78 [==============================] - 0s 190us/step - loss: 8.7859 - acc: 0.9231 - val_loss: 10.3318 - val_acc: 0.8750\n",
      "Epoch 10/20\n",
      "78/78 [==============================] - 0s 217us/step - loss: 8.5636 - acc: 0.9359 - val_loss: 10.1158 - val_acc: 0.8750\n",
      "Epoch 11/20\n",
      "78/78 [==============================] - 0s 177us/step - loss: 8.2629 - acc: 0.9103 - val_loss: 9.9155 - val_acc: 0.8750\n",
      "Epoch 12/20\n",
      "78/78 [==============================] - 0s 177us/step - loss: 7.9992 - acc: 0.9615 - val_loss: 10.1019 - val_acc: 0.7500\n",
      "Epoch 13/20\n",
      "78/78 [==============================] - 0s 205us/step - loss: 7.9554 - acc: 0.9231 - val_loss: 9.5274 - val_acc: 0.8750\n",
      "Epoch 14/20\n",
      "78/78 [==============================] - 0s 199us/step - loss: 7.4248 - acc: 1.0000 - val_loss: 9.3001 - val_acc: 0.8750\n",
      "Epoch 15/20\n",
      "78/78 [==============================] - 0s 205us/step - loss: 7.3197 - acc: 0.9872 - val_loss: 9.0635 - val_acc: 0.8750\n",
      "Epoch 16/20\n",
      "78/78 [==============================] - 0s 216us/step - loss: 7.0336 - acc: 0.9872 - val_loss: 8.8288 - val_acc: 0.8750\n",
      "Epoch 17/20\n",
      "78/78 [==============================] - 0s 190us/step - loss: 6.7421 - acc: 0.9872 - val_loss: 8.5698 - val_acc: 0.8125\n",
      "Epoch 18/20\n",
      "78/78 [==============================] - 0s 183us/step - loss: 6.4683 - acc: 1.0000 - val_loss: 8.3051 - val_acc: 0.7500\n",
      "Epoch 19/20\n",
      "78/78 [==============================] - 0s 198us/step - loss: 6.2421 - acc: 1.0000 - val_loss: 8.0497 - val_acc: 0.7500\n",
      "Epoch 20/20\n",
      "78/78 [==============================] - 0s 166us/step - loss: 6.0101 - acc: 1.0000 - val_loss: 7.7921 - val_acc: 0.8750\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dnn = build_dnn(NUM_FEATURES, NUM_LABELS)\n",
    "history = train_model(dnn, X_train, y_train, X_test, y_test, epochs=20, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> AD vs CN\n",
    "\n",
    "loss: 3.4295 - acc: 0.8846 - val_loss: 3.9303 - val_acc: 0.8125 - w/ ICV\n",
    "val_acc = 0.875 -w/o ICV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[5 0]\n",
      " [2 9]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_confusion(model, X_test, y_test):\n",
    "    y_pred_probs = dnn.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    y_true = np.argmax(hot_encoder.transform(y_test.values.reshape(-1,1)), axis=1)\n",
    "    plot_confusion_matrix(y_true, y_pred, classes=hot_encoder.categories_[0])\n",
    "make_confusion(dnn, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AD', 'CN'], dtype=object)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hot_encoder.categories_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using K=10 Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2 19 23 34 42 49 64 71]\n",
      "Original:  (68, 148)\n",
      "After Data Augmentation:  (90, 148)\n",
      "Scores on test set: loss=1.000 accuracy=0.7500\n",
      "[25 33 35 36 40 47 63 74]\n",
      "Original:  (68, 148)\n",
      "After Data Augmentation:  (90, 148)\n",
      "Scores on test set: loss=1.000 accuracy=0.7500\n",
      "[ 3  5  6 10 17 45 58 61]\n",
      "Original:  (68, 148)\n",
      "After Data Augmentation:  (90, 148)\n",
      "Scores on test set: loss=1.000 accuracy=0.7500\n",
      "[ 8 13 18 21 43 67 72 73]\n",
      "Original:  (68, 148)\n",
      "After Data Augmentation:  (90, 148)\n",
      "Scores on test set: loss=1.000 accuracy=0.8750\n",
      "[11 14 15 22 32 44 48 57]\n",
      "Original:  (68, 148)\n",
      "After Data Augmentation:  (90, 148)\n",
      "Scores on test set: loss=1.000 accuracy=0.7500\n",
      "[ 0  7 38 50 54 55 56 70]\n",
      "Original:  (68, 148)\n",
      "After Data Augmentation:  (90, 148)\n",
      "Scores on test set: loss=1.000 accuracy=0.7500\n",
      "[ 1 16 28 41 51 60 66]\n",
      "Original:  (69, 148)\n",
      "After Data Augmentation:  (90, 148)\n",
      "Scores on test set: loss=0.989 accuracy=0.8571\n",
      "[ 4 29 31 53 59 65 69]\n",
      "Original:  (69, 148)\n",
      "After Data Augmentation:  (90, 148)\n",
      "Scores on test set: loss=1.000 accuracy=1.0000\n",
      "[12 24 27 30 37 46 75]\n",
      "Original:  (69, 148)\n",
      "After Data Augmentation:  (90, 148)\n",
      "Scores on test set: loss=1.000 accuracy=0.8571\n",
      "[ 9 20 26 39 52 62 68]\n",
      "Original:  (69, 148)\n",
      "After Data Augmentation:  (90, 148)\n",
      "Scores on test set: loss=1.000 accuracy=0.7143\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold as KFold\n",
    "\n",
    "\n",
    "def getKF(X,y, n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42 ) #Default = 10\n",
    "\n",
    "    for train_index, test_index in kf.split(X,y):\n",
    "        X_train = X[train_index]\n",
    "        y_train = y.iloc[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_test = y.iloc[test_index]\n",
    "        \n",
    "        yield X_train, y_train, X_test, y_test, test_index\n",
    "\n",
    "histories = []\n",
    "testing_indxs =[]\n",
    "predictions = []\n",
    "true_labels = []\n",
    "zoo = []\n",
    "for X_train, y_train, X_test, y_test, test_index in getKF(features, labels):\n",
    "    print(test_index)\n",
    "    dnn = build_dnn(NUM_FEATURES, NUM_LABELS)\n",
    "    history = train_model(dnn,X_train, y_train, X_test, y_test, verbose=0, plot=False, epochs=100, batch_size=10)\n",
    "    \n",
    "    # Updating all information arrays\n",
    "    histories.append(history)\n",
    "    testing_indxs.append(test_index)\n",
    "    zoo.append(dnn)\n",
    "    \n",
    "    y_pred_probs = dnn.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    y_true = np.argmax(hot_encoder.transform(y_test.values.reshape(-1,1)), axis=1)\n",
    "    \n",
    "    predictions.extend(y_pred)\n",
    "    true_labels.extend(y_true)\n",
    "    \n",
    "    print(\"Scores on test set: loss={:0.3f} accuracy={:.4f}\".format(history.history[\"acc\"][-1], history.history[\"val_acc\"][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[18  7]\n",
      " [ 8 43]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x176344d30>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_confusion_matrix(predictions, true_labels, classes=hot_encoder.categories_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[0.72       0.28      ]\n",
      " [0.15686275 0.84313725]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x17c1b46d8>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_confusion_matrix(predictions, true_labels, classes=hot_encoder.categories_[0], normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(len(histories)//2,2, figsize=(20,20))\n",
    "axs=axs.flatten()\n",
    "dfs = []\n",
    "\n",
    "for i,history in enumerate(histories):\n",
    "    df = pd.DataFrame(history.history)\n",
    "    dfs.append(df)\n",
    "    df[[\"acc\",\"val_acc\"]].plot(ax=axs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_accs = [df[\"val_acc\"].iloc[-1] for df in dfs]\n",
    "# print(\"Average:\",np.mean(val_accs))\n",
    "# plt.bar(x=range(10),height=val_accs)\n",
    "# # plt.scatter(x=range(10), y=np.mean(val_accs))\n",
    "# plt.xlabel(\"Fold Number\")\n",
    "# plt.ylabel(\"Acc\")\n",
    "# plt.title(\"Accuracies for Each Fold\")\n",
    "# val_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets pick the best model from the folds..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 0s 59us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3019657919281407, 0.9736842136634024]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = -2\n",
    "# from keras import backend as K\n",
    "# K.clear_session()\n",
    "best_dnn = zoo[idx]\n",
    "# validation = labels.iloc[testing_indxs[-3]]\n",
    "# best_dnn.save(\"best_dnn.h5\")\n",
    "best_dnn.evaluate(features,hot_encoder.transform(labels.values.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating on non SMOTE Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: [array(['AD', 'CN'], dtype=object)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(74,)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def get_correct_predictions(model,X,y):\n",
    "\n",
    "model = best_dnn\n",
    "\n",
    "samples = features\n",
    "\n",
    "# hot_enc = OneHotEncoder(categories=\"auto\", sparse=False)\n",
    "sample_labels = hot_encoder.transform([[label] for label in labels])\n",
    "print(\"Categories:\", hot_encoder.categories_)\n",
    "\n",
    "predictions = model.predict(samples)\n",
    "preds = np.array([np.argmax(x) for x in predictions])\n",
    "true_labels = np.array([np.argmax(x) for x in sample_labels])\n",
    "\n",
    "correct = preds == true_labels\n",
    "AD_Sample = true_labels == 0\n",
    "\n",
    "correct_preds = preds[correct]\n",
    "correct_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SANITY CHECK\n",
      "74/74 [==============================] - 0s 52us/step\n",
      "Scores on test set: loss=0.281 accuracy=1.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"SANITY CHECK\")\n",
    "loss_and_metrics = model.evaluate(samples[correct], sample_labels[correct])\n",
    "print(\"Scores on test set: loss={:0.3f} accuracy={:.4f}\".format(*loss_and_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import innvestigate\n",
    "import innvestigate.utils as iutils\n",
    "\n",
    "\n",
    "def perform_analysis(model, analyzer, data, labels):\n",
    "    analysis = analyzer.analyze(data)\n",
    "    prediction = model.predict(data)\n",
    "    \n",
    "    df_anal = pd.DataFrame(analysis)\n",
    "    \n",
    "    return df_anal\n",
    "\n",
    "# Selecting a DNN\n",
    "test_dnn = dnn\n",
    "\n",
    "# Stripping the softmax activation from the model\n",
    "model_wo_sm = iutils.keras.graph.model_wo_softmax(test_dnn)\n",
    "\n",
    "# Creating an analyzer\n",
    "# lrp = innvestigate.create_analyzer(\"lrp.z\", model_wo_sm)\n",
    "lrp = innvestigate.analyzer.relevance_based.relevance_analyzer.LRPAlpha2Beta1(model=model_wo_sm)\n",
    "\n",
    "# Getting correctly predicted samples with AD\n",
    "test_idx = correct & AD_Sample    \n",
    "test_samples = samples[test_idx] \n",
    "test_labels = sample_labels[test_idx]\n",
    "\n",
    "AD_lrp = perform_analysis(dnn,lrp,test_samples, test_labels)\n",
    "\n",
    "# test_idx = correct & (~AD_Sample)    \n",
    "# test_samples = samples[test_idx] \n",
    "# test_labels = sample_labels[test_idx]\n",
    "\n",
    "# # perform_analysis(nn,gradient_analyzer,flowers,types)\n",
    "# CN_lrp = perform_analysis(dnn,lrp,test_samples, test_labels)\n",
    "\n",
    "# Getting all the samples that can be correctly predicted\n",
    "test_idx = correct\n",
    "all_samples = samples[test_idx] \n",
    "all_labels = sample_labels[test_idx]\n",
    "\n",
    "# perform_analysis(nn,gradient_analyzer,flowers,types)\n",
    "all_lrp = perform_analysis(dnn,lrp, all_samples, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26, 148)\n"
     ]
    }
   ],
   "source": [
    "# print(AD_lrp.shape)\n",
    "# print(CN_lrp.shape)\n",
    "print(AD_lrp.shape)\n",
    "# ALL_lrp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrp_results = all_lrp\n",
    "# population = lrp_results.mean()\n",
    "# population.plot()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x143416828>"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average for entire population\n",
    "plt.figure()\n",
    "all_lrp.mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_features = population.sort_values(ascending=False)\n",
    "best_features = sorted_features[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([96, 94, 46, 22, 110, 134], dtype='int64')"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_features.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abs(lrp_results)[best_features.index].hist(figsize=(20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['G_oc-temp_med-Parahip_TH_rh', 'G_oc-temp_lat-fusifor_TH_rh',\n",
       "       'S_circular_insula_ant_TH_lh', 'G_oc-temp_med-Parahip_TH_lh',\n",
       "       'G_temporal_inf_TH_rh', 'S_oc-temp_med_and_Lingual_TH_rh'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_features.columns[best_features.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "         65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "         91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "        104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "        130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "        143, 144, 145, 146, 147]), <a list of 148 Text xticklabel objects>)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "sorted_features.plot(kind=\"bar\", figsize=[20,10])\n",
    "plt.xticks(rotation=65, fontsize=\"small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pos_only = lrp_results.copy()\n",
    "pos_only[pos_only < 0] = 0\n",
    "pca.fit(pos_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance:  [0.38462503 0.10845578]\n"
     ]
    }
   ],
   "source": [
    "print(\"Variance: \", pca.explained_variance_ratio_)\n",
    "# pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 75 entries, 0 to 74\n",
      "Data columns (total 2 columns):\n",
      "PC1    75 non-null float64\n",
      "PC2    75 non-null float64\n",
      "dtypes: float64(2)\n",
      "memory usage: 1.2 KB\n"
     ]
    }
   ],
   "source": [
    "X = pca.transform(pos_only)\n",
    "df = pd.DataFrame(X, columns=[\"PC1\",\"PC2\"])\n",
    "_labels = np.array([np.argmax(x) for x in all_labels])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "760c0382f5a1421eb68ec58fb9bbaad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: [array(['AD', 'CN'], dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "# plt.figure(figsize=(20,10))\n",
    "df.plot.scatter(x=\"PC1\", y=\"PC2\", s= 30, c=_labels, colormap='winter',figsize=(12,8))\n",
    "# plt.legend([\"AD\", \"CN\"])\n",
    "print(\"Categories:\", hot_encoder.categories_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot PCA for 3 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance:  [0.38462503 0.10845578 0.09197229]\n"
     ]
    }
   ],
   "source": [
    "pca3 = PCA(n_components=3)\n",
    "pca3.fit(pos_only)\n",
    "print(\"Variance: \", pca3.explained_variance_ratio_)\n",
    "\n",
    "pc_3d = pca3.transform(pos_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883b5aadb37d4aa19ea43de8e42ee96d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "fig = plt.figure(figsize=(12,10))\n",
    "ax = fig.add_subplot(111, projection='3d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.scatter(pc_3d[:,0], pc_3d[:,1], pc_3d[:,2], c=_labels, s=40)\n",
    "ax.set(xlabel=\"PC1\", ylabel=\"PC2\", zlabel=\"PC3\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "# pca_reduced = PCA(n_components=50)\n",
    "tSNE = TSNE(n_components=2, init=\"pca\", random_state=42)\n",
    "tSNE_relevance = tSNE.fit_transform(pos_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "823e10bb271c435dacc33c46a4e384e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(75, 2)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tSNE_relevance\n",
    "fig = plt.figure(figsize=(12,10))\n",
    "tSNE_relevance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x17d224748>"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.scatter(x=X[:,0], y=X[:,1], s= 30, c=_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condaflow",
   "language": "python",
   "name": "condaflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
