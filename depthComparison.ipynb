{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "from helper import *\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a DNN on the modified dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b6542613b84d379f71445b35a2e394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: (4500, 2)\n",
      "Test Size: (500,)\n",
      "Categories: [array([0, 1])]\n"
     ]
    }
   ],
   "source": [
    "# Get split returns a generator\n",
    "# List comprehension is one way to evaluate a generator\n",
    "\n",
    "original_data, modded_samples, training_labels, original_labels = simulate_blobs(class_size=5000)\n",
    "\n",
    "# Separating a hold out set that will be used for validation later\n",
    "X_train, y_train, X_test, y_test, y_original, X_valid, y_valid, y_valid_original = get_train_test_val(modded_samples, original_labels, training_labels)\n",
    "\n",
    "\n",
    "print(\"Train Size:\", X_train.shape)\n",
    "print(\"Test Size:\", y_test.shape)\n",
    "\n",
    "\n",
    "hot_encoder = dfHotEncoder()\n",
    "hot_encoder.fit(training_labels)\n",
    "print(\"Categories:\", hot_encoder.categories_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEATURES = X_train.shape[1]\n",
    "NUM_LABELS = len(hot_encoder.categories_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dnn(num_features, num_nodes = 16, depth = 2, num_labels=2, activation = \"elu\"):\n",
    "    \n",
    "    keras.backend.clear_session()\n",
    "\n",
    "    nn = keras.models.Sequential()\n",
    "    Dense = keras.layers.Dense\n",
    "    \n",
    "    # Using He initialization\n",
    "    he_init = tf.keras.initializers.he_uniform()\n",
    "    \n",
    "    nn.add(Dense(units = num_nodes, activation=activation, input_dim=num_features,\n",
    "                kernel_initializer=he_init))\n",
    "    \n",
    "    for i in range(1,depth):\n",
    "        nn.add(Dense(units = num_nodes, activation=activation,\n",
    "                    kernel_initializer=he_init))\n",
    "\n",
    "    nn.add(Dense(units=num_labels, activation= \"softmax\",\n",
    "                kernel_initializer=he_init))\n",
    "    \n",
    "    nn.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer='sgd',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return nn\n",
    "\n",
    "def train_model(model, X, y, epochs=30, batch_size=20, verbose=0):\n",
    "    \n",
    "    ZScaler = StandardScaler().fit(X)\n",
    "    \n",
    "    X_train = ZScaler.transform(X)\n",
    "    y_train = hot_encoder.transform(y)\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size = batch_size, verbose=verbose)\n",
    "    \n",
    "    return history, ZScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "4500/4500 [==============================] - 0s 66us/step - loss: 0.2082 - acc: 0.9687 - val_loss: 0.0801 - val_acc: 1.0000\n",
      "Epoch 2/50\n",
      "4500/4500 [==============================] - 0s 33us/step - loss: 0.0525 - acc: 1.0000 - val_loss: 0.0373 - val_acc: 1.0000\n",
      "Epoch 3/50\n",
      "4500/4500 [==============================] - 0s 32us/step - loss: 0.0281 - acc: 1.0000 - val_loss: 0.0232 - val_acc: 1.0000\n",
      "Epoch 4/50\n",
      "4500/4500 [==============================] - 0s 32us/step - loss: 0.0186 - acc: 1.0000 - val_loss: 0.0165 - val_acc: 1.0000\n",
      "Epoch 5/50\n",
      "4500/4500 [==============================] - 0s 33us/step - loss: 0.0137 - acc: 1.0000 - val_loss: 0.0126 - val_acc: 1.0000\n",
      "Epoch 6/50\n",
      "4500/4500 [==============================] - 0s 33us/step - loss: 0.0107 - acc: 1.0000 - val_loss: 0.0102 - val_acc: 1.0000\n",
      "Epoch 7/50\n",
      "4500/4500 [==============================] - 0s 34us/step - loss: 0.0087 - acc: 1.0000 - val_loss: 0.0085 - val_acc: 1.0000\n",
      "Epoch 8/50\n",
      "4500/4500 [==============================] - 0s 33us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 9/50\n",
      "4500/4500 [==============================] - 0s 35us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 0.0063 - val_acc: 1.0000\n",
      "Epoch 10/50\n",
      "4500/4500 [==============================] - 0s 33us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.0055 - val_acc: 1.0000\n",
      "Epoch 11/50\n",
      "4500/4500 [==============================] - 0s 35us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.0050 - val_acc: 1.0000\n",
      "Epoch 12/50\n",
      "4500/4500 [==============================] - 0s 32us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 1.0000\n",
      "Epoch 13/50\n",
      "4500/4500 [==============================] - 0s 33us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 0.0041 - val_acc: 1.0000\n",
      "Epoch 14/50\n",
      "4500/4500 [==============================] - 0s 33us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "Epoch 15/50\n",
      "4500/4500 [==============================] - 0s 34us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.0034 - val_acc: 1.0000\n",
      "Epoch 16/50\n",
      "4500/4500 [==============================] - 0s 40us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "Epoch 17/50\n",
      "4500/4500 [==============================] - 0s 34us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.0030 - val_acc: 1.0000\n",
      "Epoch 18/50\n",
      "4500/4500 [==============================] - 0s 33us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "Epoch 19/50\n",
      "4500/4500 [==============================] - 0s 35us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 20/50\n",
      "4500/4500 [==============================] - 0s 36us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.0025 - val_acc: 1.0000\n",
      "Epoch 21/50\n",
      "4500/4500 [==============================] - 0s 35us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 22/50\n",
      "4500/4500 [==============================] - 0s 38us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 23/50\n",
      "4500/4500 [==============================] - 0s 36us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 24/50\n",
      "4500/4500 [==============================] - 0s 34us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 25/50\n",
      "4500/4500 [==============================] - 0s 35us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 26/50\n",
      "4500/4500 [==============================] - 0s 38us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 27/50\n",
      "4500/4500 [==============================] - 0s 35us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 28/50\n",
      "4500/4500 [==============================] - 0s 33us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 1.0000\n",
      "Epoch 29/50\n",
      "4500/4500 [==============================] - 0s 36us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 1.0000\n",
      "Epoch 30/50\n",
      "4500/4500 [==============================] - 0s 38us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 31/50\n",
      "4500/4500 [==============================] - 0s 34us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 32/50\n",
      "4500/4500 [==============================] - 0s 34us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 33/50\n",
      "4500/4500 [==============================] - 0s 34us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
      "Epoch 34/50\n",
      "4500/4500 [==============================] - 0s 35us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 35/50\n",
      "4500/4500 [==============================] - 0s 33us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 36/50\n",
      "4500/4500 [==============================] - 0s 37us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 37/50\n",
      "4500/4500 [==============================] - 0s 34us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 38/50\n",
      "4500/4500 [==============================] - 0s 35us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 39/50\n",
      "4500/4500 [==============================] - 0s 36us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 40/50\n",
      "4500/4500 [==============================] - 0s 36us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 41/50\n",
      "4500/4500 [==============================] - 0s 36us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 42/50\n",
      "4500/4500 [==============================] - 0s 34us/step - loss: 9.8211e-04 - acc: 1.0000 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 43/50\n",
      "4500/4500 [==============================] - 0s 34us/step - loss: 9.5568e-04 - acc: 1.0000 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 44/50\n",
      "4500/4500 [==============================] - 0s 35us/step - loss: 9.3066e-04 - acc: 1.0000 - val_loss: 9.9467e-04 - val_acc: 1.0000\n",
      "Epoch 45/50\n",
      "4500/4500 [==============================] - 0s 34us/step - loss: 9.0679e-04 - acc: 1.0000 - val_loss: 9.6956e-04 - val_acc: 1.0000\n",
      "Epoch 46/50\n",
      "4500/4500 [==============================] - 0s 42us/step - loss: 8.8404e-04 - acc: 1.0000 - val_loss: 9.4554e-04 - val_acc: 1.0000\n",
      "Epoch 47/50\n",
      "4500/4500 [==============================] - 0s 47us/step - loss: 8.6237e-04 - acc: 1.0000 - val_loss: 9.2267e-04 - val_acc: 1.0000\n",
      "Epoch 48/50\n",
      "4500/4500 [==============================] - 0s 42us/step - loss: 8.4174e-04 - acc: 1.0000 - val_loss: 9.0093e-04 - val_acc: 1.0000\n",
      "Epoch 49/50\n",
      "4500/4500 [==============================] - 0s 34us/step - loss: 8.2199e-04 - acc: 1.0000 - val_loss: 8.8010e-04 - val_acc: 1.0000\n",
      "Epoch 50/50\n",
      "4500/4500 [==============================] - 0s 35us/step - loss: 8.0308e-04 - acc: 1.0000 - val_loss: 8.6012e-04 - val_acc: 1.0000\n",
      "CPU times: user 13.6 s, sys: 4.01 s, total: 17.6 s\n",
      "Wall time: 8.22 s\n"
     ]
    }
   ],
   "source": [
    "nn = build_dnn(NUM_FEATURES)\n",
    "%time history, Zscaler = train_model(nn, X_train, y_train, X_test, y_test, epochs=50, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e3ef25eea8344a1bf0ea78e646e1a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting results from history\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 16us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15590375757217406, 0.926]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.evaluate(Zscaler.transform(X_test),hot_encoder.transform(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing LRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy\n",
      "5000/5000 [==============================] - 0s 8us/step\n",
      "Scores on data set: loss=0.001 accuracy=1.0000\n"
     ]
    }
   ],
   "source": [
    "import innvestigate\n",
    "import innvestigate.utils as iutils\n",
    "\n",
    "plot_args = {\"kind\":\"scatter\", \"x\":0,  \"y\":1, \"c\":\"label\", \"cmap\": \"Set1\", \"s\":10, \"alpha\":0.25}\n",
    "\n",
    "def perform_analysis(model, analyzer, data, labels=[]):\n",
    "    analysis = analyzer.analyze(data)\n",
    "    prediction = model.predict(data)\n",
    "    \n",
    "    df_anal = pd.DataFrame(analysis)\n",
    "    \n",
    "    return df_anal\n",
    "\n",
    "\n",
    "scaled_samples = Zscaler.transform(X_valid)\n",
    "\n",
    "# Getting all the samples that can be correctly predicted\n",
    "all_samples, _labels, correct_idxs = getCorrectPredictions(nn, scaled_samples, y_valid, enc = hot_encoder)\n",
    "all_labels = y_valid_original[correct_idxs]\n",
    "\n",
    "\n",
    "# Stripping the softmax activation from the model\n",
    "model_w_softmax = nn\n",
    "model = iutils.keras.graph.model_wo_softmax(model_w_softmax)\n",
    "\n",
    "# Creating an analyzer\n",
    "lrp_E = innvestigate.analyzer.relevance_based.relevance_analyzer.LRPEpsilon(model=model, epsilon=1e-3)\n",
    "# lrp_Z = innvestigate.analyzer.relevance_based.relevance_analyzer.LRPZPlus(model=model)\n",
    "\n",
    "all_lrp_E = perform_analysis(model,lrp_E, all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607dfd81999249a5b003813eead7af47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close(\"Positive Only LRP\")\n",
    "fig, axs = plt.subplots(1,3, figsize=(18,6), num=\"Positive Only LRP\")\n",
    "\n",
    "plot_args[\"c\"] = \"label\"\n",
    "original_data.plot(ax=axs[0], title=\"Original Distribution\", **plot_args)\n",
    "\n",
    "plot_args[\"c\"] = all_labels\n",
    "all_lrp_E.plot(ax=axs[1], title=\"LRP E\", **plot_args)\n",
    "\n",
    "pos_lrp = all_lrp_E.copy()\n",
    "pos_lrp[pos_lrp<0] = 0\n",
    "pos_lrp[\"label\"] = all_labels.values\n",
    "pos_lrp.plot(ax=axs[2],title=\"LRP E\", **plot_args)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peforming an analysis on depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa7273b66c3495faad534f6a7409d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "depth_epochs = {2:50,3:50,4:100,5:100}\n",
    "\n",
    "\n",
    "def runDNN(depth, epochs):\n",
    "    nn = build_dnn(NUM_FEATURES)\n",
    "    history, ZScaler = train_model(nn, X_train, y_train, \n",
    "                                   epochs=epochs, batch_size=20)\n",
    "    \n",
    "    \n",
    "    scaled_samples = ZScaler.transform(X_valid)\n",
    "    final_acc = nn.evaluate(scaled_samples,hot_encoder.transform(y_test))\n",
    "\n",
    "    # Getting all the samples that can be correctly predicted\n",
    "    all_samples, _labels, correct_idxs = getCorrectPredictions(nn, scaled_samples, y_valid, enc = hot_encoder)\n",
    "    lrp_labels = y_valid_original[correct_idxs]\n",
    "\n",
    "\n",
    "    # Stripping the softmax activation from the model\n",
    "    model_w_softmax = nn\n",
    "    model = iutils.keras.graph.model_wo_softmax(model_w_softmax)\n",
    "\n",
    "    # Creating an analyzer\n",
    "    lrp_E = innvestigate.analyzer.relevance_based.relevance_analyzer.LRPEpsilon(model=model, epsilon=1e-3)\n",
    "    lrp_results = perform_analysis(model,lrp_E, all_samples)\n",
    "    \n",
    "    return (final_acc, lrp_results, lrp_labels)\n",
    "\n",
    "plt.close(\"Depth Comparison\")\n",
    "fig, axs = plt.subplots(len(depths),1, figsize=(15,8*len(depths)), num=\"Depth Comparison\")\n",
    "plt.tight_layout()\n",
    "\n",
    "_labels = all_labels.values\n",
    "\n",
    "# depth_epochs = {2:50}\n",
    "\n",
    "# results = runDNN()\n",
    "\n",
    "for i,d in enumerate(depth_epochs):\n",
    "    \n",
    "    final_acc, \n",
    "    \n",
    "    pos_lrp = lrp_results.copy()\n",
    "    pos_lrp[pos_lrp<0] = 0\n",
    "    data = pos_lrp.values\n",
    "\n",
    "    axs[i].scatter(*data.T, s=50, linewidth=0, c=lrp_labels, alpha=0.5, cmap=plot_args[\"cmap\"])\n",
    "    axs[i].set_title(\"Depth: {}\".format(d))\n",
    "\n",
    "\n",
    "# pos_lrp[\"label\"] = all_labels.values\n",
    "# pos_lrp.plot(ax=axs[2],title=\"LRP E\", **plot_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condaflow",
   "language": "python",
   "name": "condaflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
